{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "This jupyter notebook creates an sklearn pipeline for our data preprocessing. The steps that we need to follow are motivated by the EDA and are split into\n",
    "\n",
    "- Numerical features\n",
    "- High-cardinality categorical features\n",
    "- Low-cardinality categorical features\n",
    "- Boolean categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data has 800000 rows and 32 columns.\n",
      "The testing data has 200000 rows and 32 columns.\n"
     ]
    }
   ],
   "source": [
    "# Data collection\n",
    "total_df = pd.read_csv('./Data/Base.csv')\n",
    "\n",
    "# Split the DataFrame into training and test sets using stratified sampling to maintain anomaly distribution\n",
    "train_df, test_df = train_test_split(total_df, test_size=0.2, stratify=total_df['fraud_bool'], random_state=42)\n",
    "\n",
    "# Validate the size of the data\n",
    "train_shape = train_df.shape\n",
    "test_shape = test_df.shape\n",
    "print(f\"The training data has {train_shape[0]} rows and {train_shape[1]} columns.\")\n",
    "print(f\"The testing data has {test_shape[0]} rows and {test_shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations for numerical features\n",
    "\n",
    "The numerical features will be transformed as follows\n",
    "\n",
    "Name_email_similarity:\n",
    "- Convert into 5 bins, 4 with range 0.24 and 1 with range 0.04 (0.96-1)\n",
    "- There are no outliers\n",
    "\n",
    "Days_since_request:\n",
    "- Use a log transform and then remove outliers\n",
    "- Add boolean feature called is_days_since_request_outlier \n",
    "\n",
    "Intended_balcon_amount\n",
    "- Use a log transform and then remove outliers\n",
    "- Add boolean feature called has_positive_account\n",
    "\n",
    "Velocity_6h, _24h, 4w\n",
    "- Remove all\n",
    "\n",
    "Session_length_in_minutes\n",
    "- Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   days_since_request_outlier  intended_balcon_amount_positive\n",
      "0                         0.0                              1.0\n",
      "1                         0.0                              1.0\n",
      "2                         0.0                              0.0\n",
      "3                         0.0                              0.0\n",
      "4                         0.0                              0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pandas/core/internals/blocks.py:402: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = func(self.values, **kwargs)\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/extmath.py:980: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/extmath.py:985: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/extmath.py:1005: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_email_similarity_binned</th>\n",
       "      <th>days_since_request_outlier</th>\n",
       "      <th>days_since_request_transformed_scaled</th>\n",
       "      <th>intended_balcon_amount_positive</th>\n",
       "      <th>intended_balcon_amount_transformed_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.358111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.318824</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.687875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.873950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.533002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name_email_similarity_binned  days_since_request_outlier  \\\n",
       "0                           0.0                         0.0   \n",
       "1                           3.0                         0.0   \n",
       "2                           2.0                         0.0   \n",
       "3                           3.0                         0.0   \n",
       "4                           4.0                         0.0   \n",
       "\n",
       "   days_since_request_transformed_scaled  intended_balcon_amount_positive  \\\n",
       "0                               0.358111                              1.0   \n",
       "1                              -0.318824                              1.0   \n",
       "2                               1.687875                              0.0   \n",
       "3                              -0.873950                              0.0   \n",
       "4                              -0.533002                              0.0   \n",
       "\n",
       "   intended_balcon_amount_transformed_scaled  \n",
       "0                                        NaN  \n",
       "1                                        NaN  \n",
       "2                                        NaN  \n",
       "3                                        NaN  \n",
       "4                                        NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom binning function with dynamic bins\n",
    "def binner(X, bins):\n",
    "    return np.digitize(X, bins=bins, right=False) - 1\n",
    "\n",
    "# Create boolean feature for positive threshold\n",
    "def bool_from_positives(X):\n",
    "    return (X > 0).astype(int)\n",
    "\n",
    "# Log transformation\n",
    "def log_transform(X):\n",
    "    return np.log1p(X)\n",
    "\n",
    "# Clipping outliers using IQR\n",
    "def clip_outliers(X):\n",
    "    Q1, Q3 = np.percentile(X, [25, 75], axis=0)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return np.minimum(X, upper_bound)\n",
    "\n",
    "# Create outlier indicator (Ensures output is 0 or 1)\n",
    "def create_outlier_indicator(X):\n",
    "    Q1, Q3 = np.percentile(X, [25, 75], axis=0)\n",
    "    IQR = Q3 - Q1\n",
    "    threshold = Q3 + 1.5 * IQR\n",
    "    return (X > threshold).astype(int)\n",
    "\n",
    "# Define bins for name_email_similarity\n",
    "similarity_bins = np.array([0.0, 0.24, 0.48, 0.72, 0.96, 1.0])\n",
    "\n",
    "# Pipeline for name_email_similarity\n",
    "name_email_similarity_pipeline = Pipeline(steps=[\n",
    "    ('binner', FunctionTransformer(func=lambda X: binner(X, bins=similarity_bins)))\n",
    "])\n",
    "\n",
    "# Pipeline for days_since_request outlier indicator\n",
    "days_since_request_outlier_pipeline = Pipeline(steps=[\n",
    "    ('outlier_indicator', FunctionTransformer(func=create_outlier_indicator))\n",
    "])\n",
    "\n",
    "# Pipeline for days_since_request numerical transformations\n",
    "days_since_request_numeric_pipeline = Pipeline(steps=[\n",
    "    ('log_transform', FunctionTransformer(func=log_transform)),\n",
    "    ('clip_outliers', FunctionTransformer(func=clip_outliers)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for intended_balcon_amount positive indicator\n",
    "intended_balcon_amount_positive_pipeline = Pipeline(steps=[\n",
    "    ('positive_indicator', FunctionTransformer(func=bool_from_positives))\n",
    "])\n",
    "\n",
    "# Pipeline for intended_balcon_amount numerical transformations\n",
    "intended_balcon_amount_numeric_pipeline = Pipeline(steps=[\n",
    "    ('log_transform', FunctionTransformer(func=log_transform)),\n",
    "    ('clip_outliers', FunctionTransformer(func=clip_outliers)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine all pipelines into a ColumnTransformer\n",
    "numerical_preprocessor = ColumnTransformer(transformers=[\n",
    "    # Binning for name_email_similarity\n",
    "    ('name_email_similarity_binned', name_email_similarity_pipeline, ['name_email_similarity']),\n",
    "    \n",
    "    # Outlier indicator for days_since_request\n",
    "    ('days_since_request_outlier', days_since_request_outlier_pipeline, ['days_since_request']),\n",
    "    \n",
    "    # Numerical transformations for days_since_request\n",
    "    ('days_since_request_numeric', days_since_request_numeric_pipeline, ['days_since_request']),\n",
    "    \n",
    "    # Positive indicator for intended_balcon_amount\n",
    "    ('intended_balcon_amount_positive', intended_balcon_amount_positive_pipeline, ['intended_balcon_amount']),\n",
    "    \n",
    "    # Numerical transformations for intended_balcon_amount\n",
    "    ('intended_balcon_amount_numeric', intended_balcon_amount_numeric_pipeline, ['intended_balcon_amount'])\n",
    "])\n",
    "\n",
    "# Fit and transform the data\n",
    "preprocessed_train_data = numerical_preprocessor.fit_transform(train_df)\n",
    "\n",
    "# Define appropriate column names based on the transformations applied\n",
    "new_columns = [\n",
    "    'name_email_similarity_binned',               # From the binner in name_email_similarity_pipeline\n",
    "    'days_since_request_outlier',                 # Boolean outlier indicator\n",
    "    'days_since_request_transformed_scaled',      # Log transformed, clipped, and scaled days_since_request\n",
    "    'intended_balcon_amount_positive',            # Boolean positive indicator\n",
    "    'intended_balcon_amount_transformed_scaled'   # Log transformed, clipped, and scaled intended_balcon_amount\n",
    "]\n",
    "\n",
    "# Convert the preprocessed data (NumPy array) to a DataFrame\n",
    "preprocessed_train_df = pd.DataFrame(preprocessed_train_data, columns=new_columns)\n",
    "\n",
    "# Show the complete preprocessed DataFrame\n",
    "preprocessed_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations for high-cardinality categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations for low-cardinality categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations for boolean categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOULD BE MOVED TO DATAPREPROCESSING?\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "encoded_train_df = pd.get_dummies(train_df, columns=cat_df.columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOULD BE MOVED TO DATAPREPROCESSING?\n",
    "\n",
    "# Apply log transformation\n",
    "for column in ['velocity_6h', 'velocity_24h', 'zip_count_4w']:\n",
    "    train_df[column] = np.log1p(train_df[column])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
