{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "This jupyter notebook creates an sklearn pipeline for our data preprocessing. The steps that we need to follow are motivated by the EDA and are split into\n",
    "\n",
    "- Numerical features\n",
    "- High-cardinality categorical features\n",
    "- Low-cardinality categorical features\n",
    "- Boolean categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import Binarizer\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data has 800000 rows and 32 columns.\n",
      "The testing data has 200000 rows and 32 columns.\n"
     ]
    }
   ],
   "source": [
    "# Data collection\n",
    "total_df = pd.read_csv('./Data/Base.csv')\n",
    "\n",
    "# Split the DataFrame into training and test sets using stratified sampling to maintain anomaly distribution\n",
    "train_df, test_df = train_test_split(total_df, test_size=0.2, stratify=total_df['fraud_bool'], random_state=42)\n",
    "\n",
    "# Validate the size of the data\n",
    "train_shape = train_df.shape\n",
    "test_shape = test_df.shape\n",
    "print(f\"The training data has {train_shape[0]} rows and {train_shape[1]} columns.\")\n",
    "print(f\"The testing data has {test_shape[0]} rows and {test_shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations for numerical features\n",
    "\n",
    "The numerical features will be transformed as follows\n",
    "\n",
    "Name_email_similarity:\n",
    "- Convert into 5 bins, 4 with range 0.24 and 1 with range 0.04 (0.96-1)\n",
    "- There are no outliers\n",
    "\n",
    "Days_since_request:\n",
    "- Convert into 3 bins [0->Q2, Q2->Q3+1.5* IQR, Q3+1.5*IQR+]\n",
    "\n",
    "Intended_balcon_amount\n",
    "- Replace with boolean feature if positive account\n",
    "\n",
    "Velocity_6h, _24h, 4w\n",
    "- Remove all\n",
    "\n",
    "Session_length_in_minutes\n",
    "- Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer for binning 'name_email_similarity'\n",
    "class NameEmailSimilarityBinner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.bin_edges = np.array([0.0, 0.24, 0.48, 0.72, 0.96, 1.0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.digitize(X.to_numpy().flatten(), bins=self.bin_edges, right=False).reshape(-1, 1)\n",
    "\n",
    "# Custom transformer for binning 'days_since_request'\n",
    "class DaysSinceRequestBinner(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        X_series = X.to_numpy().flatten()\n",
    "        self.Q2 = np.quantile(X_series, 0.5)\n",
    "        self.upper_fence = np.quantile(X_series, 0.75) + 1.5 * (np.quantile(X_series, 0.75) - np.quantile(X_series, 0.25))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        bin_edges = np.array([-np.inf, self.Q2, self.upper_fence, np.inf])\n",
    "        return np.digitize(X.to_numpy().flatten(), bins=bin_edges, right=False).reshape(-1, 1)\n",
    "\n",
    "# Pipeline to drop velocity columns and apply transformations while keeping other columns\n",
    "numerical_preprocessor = Pipeline([\n",
    "    ('drop_velocity_columns', FunctionTransformer(lambda X: X.drop(columns=[\"velocity_6h\", \"velocity_24h\", \"velocity_4w\"]))),\n",
    "    ('transformer', ColumnTransformer([\n",
    "        ('name_email_similarity_binned', NameEmailSimilarityBinner(), ['name_email_similarity']),\n",
    "        ('days_since_request_binned', DaysSinceRequestBinner(), ['days_since_request']),\n",
    "        ('intended_balcon_amount_positive', Binarizer(threshold=0.0), ['intended_balcon_amount'])\n",
    "    ], remainder='passthrough'))  # Keep other columns as they are\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations for high-cardinality categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations for low-cardinality categorical features\n",
    "The categorical features will be transformed as follows:\n",
    "\n",
    "customer_age:\n",
    "- Shows linear relationship with fraud\n",
    "- Use ordinal encoding to preserve order\n",
    "- Higher age correlates with higher fraud risk\n",
    "\n",
    "payment_type:\n",
    "- Categories: AA, AB, AC, AD, AE\n",
    "- Use one-hot encoding\n",
    "- Shows category-specific risks with AC having highest fraud rate\n",
    "\n",
    "employment_status:\n",
    "- Categories: CA, CB, CC, CD, CE, CF, CG\n",
    "- Use one-hot encoding\n",
    "- \"Unemployed\" (CD) shows highest fraud rate (~5%)\n",
    "- \"Self-employed\" (CC) shows moderate fraud rate (~2%)\n",
    "- \"Employed\" (CA) shows lowest fraud rate (~0.8%)\n",
    "\n",
    "housing_status:\n",
    "- Categories: BA, BB, BC, BD, BE, BF, BG\n",
    "- Use one-hot encoding\n",
    "- \"Other\" and \"Parents\" categories show elevated fraud rates (3-4%)\n",
    "- \"Own\" shows lowest fraud rate (~0.5%)\n",
    "\n",
    "device_os:\n",
    "- Categories: linux, macintosh, other, windows, x11\n",
    "- Use one-hot encoding\n",
    "- Windows shows slightly higher fraud rate\n",
    "- Keep all categories as they show distinct risk profiles\n",
    "\n",
    "device_distinct_emails_8w:\n",
    "- Already numeric (values: -1, 0, 1, 2)\n",
    "- Keep as is - pass through without transformation\n",
    "- Shows clear pattern with fraud rates\n",
    "\n",
    "month:\n",
    "- Categories: 0-7\n",
    "- Use one-hot encoding\n",
    "- No strong relationship with fraud\n",
    "Keep for potential seasonal patterns\n",
    "\n",
    "income:\n",
    "- U-shaped relationship with fraud\n",
    "- Create 5 bins: [0-0.2, 0.2-0.4, 0.4-0.6, 0.6-0.8, 0.8-1.0]\n",
    "- Use one-hot encoding on bins\n",
    "- Both very low and very high values show higher fraud rates (~4-5%)\n",
    "\n",
    "proposed_credit_limit:\n",
    "- U-shaped relationship with fraud\n",
    "- Create 6 bins: [0-500, 500-1000, 1000-2000, 2000-5000, 5000-10000, 10000+]\n",
    "- Use one-hot encoding on bins\n",
    "- Both very low and very high values show higher fraud rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowCardinalityPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "       \n",
    "        # one-hot encoded features\n",
    "        self.onehot_features = [\n",
    "            'payment_type',\n",
    "            'employment_status',\n",
    "            'housing_status',\n",
    "            'device_os',\n",
    "            'month',\n",
    "            'credit_bin', # proposed_credit_limit\n",
    "            'income_bin' # income\n",
    "        ]\n",
    "        self.onehot_encoder = OneHotEncoder(drop='first', handle_unknown='ignore')\n",
    "        \n",
    "        # ordinal encoded feature bc shows linear relationship\n",
    "        self.ordinal_features = ['customer_age']\n",
    "        self.ordinal_encoder = OrdinalEncoder()\n",
    "        \n",
    "        \n",
    "    def create_bins(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        # bin credit limit (U-shaped relationship)\n",
    "        credit_bins = [0, 500, 1000, 2000, 5000, 10000, float('inf')]\n",
    "        X_transformed['credit_bin'] = pd.cut(X['proposed_credit_limit'], \n",
    "                                           bins=credit_bins, \n",
    "                                           labels=['very_low', 'low', 'medium', 'high', 'very_high', 'extreme'])\n",
    "        \n",
    "        # bin income (U-shaped relationship)\n",
    "        income_bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "        X_transformed['income_bin'] = pd.cut(X['income'], \n",
    "                                           bins=income_bins, \n",
    "                                           labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "        \n",
    "        return X_transformed\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        # creating bins and fitting categorical features\n",
    "        X_binned = self.create_bins(X)\n",
    "        self.onehot_encoder.fit(X_binned[self.onehot_features])\n",
    "        self.ordinal_encoder.fit(X[self.ordinal_features])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # create bins\n",
    "        X_transformed = self.create_bins(X)\n",
    "        \n",
    "        # transform form one-hot features --> to dense array\n",
    "        onehot_encoded = self.onehot_encoder.transform(X_transformed[self.onehot_features]).toarray()\n",
    "        onehot_feature_names = self.onehot_encoder.get_feature_names_out(self.onehot_features)\n",
    "        \n",
    "        # transform ordinal feature (customer_age)\n",
    "        ordinal_encoded = self.ordinal_encoder.transform(X[self.ordinal_features])\n",
    "        \n",
    "        # combine all transformed features\n",
    "        transformed_array = np.hstack([onehot_encoded, ordinal_encoded])\n",
    "        \n",
    "        # create feature names\n",
    "        feature_names = [*onehot_feature_names, *self.ordinal_features]\n",
    "        \n",
    "        # create final dataframe\n",
    "        transformed_df = pd.DataFrame(\n",
    "            transformed_array,\n",
    "            columns=feature_names,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # separately handle device_distinct_emails_8w since it's already numeric\n",
    "        transformed_df['device_distinct_emails_8w'] = X['device_distinct_emails_8w']\n",
    "        \n",
    "        return transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations for boolean categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For feature selection, we will retain features with strong associations to fraud and drop or combine weaker predictors. Since binary features generally don’t require scaling, they can move directly into modeling, though combining weaker features may enhance their predictive utility. With this strategy, we can optimize the dataset for fraud prediction while reducing unnecessary complexity.\n",
    "\n",
    "### Summary of feature transformations for boolean features\n",
    "\n",
    "fraud_bool\n",
    "- Retain as it is the target variable\n",
    "\n",
    "email_is_free\n",
    "- Retain\n",
    "\n",
    "phone_home_valid and phone_mobile_valid\n",
    "- total_valid_phones which counts valid phone numbers (0, 1, or 2) by combining phone_home_valid and phone_mobile_valid.\n",
    "\n",
    "has_other_cards\n",
    "- Retain\n",
    "\n",
    "foreign_request and keep_alive_session\n",
    "- Combined feature foreign_long_session to check for long session times from foreign locations which might indicate unusual behavior, like accessing accounts from unfamiliar regions.\n",
    "\n",
    "source\n",
    "- One-hot encoding\n",
    "\n",
    "device_fraud_count and keep_alive_session\n",
    "- Combine to create a flag that captures both device history and session longevity, potentially signaling abnormal use patterns.\n",
    "\n",
    "total_risk_flags\n",
    "- A derived aggregated fraud risk score using high-risk indicators (email_is_free, foreign_request, has_other_cards, and keep_alive_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to create `total_valid_phones`\n",
    "class TotalValidPhonesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X['phone_home_valid'] + X['phone_mobile_valid']).values.reshape(-1, 1)\n",
    "\n",
    "# Custom transformer to create `foreign_long_session`\n",
    "class ForeignLongSessionTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X['foreign_request'] & X['keep_alive_session']).astype(int).values.reshape(-1, 1)\n",
    "\n",
    "# Custom transformer to create `device_and_account_history`\n",
    "class DeviceAccountHistoryTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X['device_fraud_count'] * X['keep_alive_session']).values.reshape(-1, 1)\n",
    "\n",
    "# Custom transformer to create `total_risk_flags`\n",
    "class TotalRiskFlagsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.high_risk_features = ['email_is_free', 'foreign_request', 'has_other_cards', 'keep_alive_session']\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.high_risk_features].sum(axis=1).values.reshape(-1, 1)\n",
    "\n",
    "# Pipeline to create new features from Boolean data\n",
    "boolean_feature_preprocessor = ColumnTransformer([\n",
    "    ('total_valid_phones', TotalValidPhonesTransformer(), ['phone_home_valid', 'phone_mobile_valid']),\n",
    "    ('foreign_long_session', ForeignLongSessionTransformer(), ['foreign_request', 'keep_alive_session']),\n",
    "    ('device_and_account_history', DeviceAccountHistoryTransformer(), ['device_fraud_count', 'keep_alive_session']),\n",
    "    ('total_risk_flags', TotalRiskFlagsTransformer(), ['email_is_free', 'foreign_request', 'has_other_cards', 'keep_alive_session']),\n",
    "    ('source_encoded', 'passthrough', ['source'])  # Assuming 'source' is preprocessed with one-hot encoding externally\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Sample implementation for preprocessing pipeline\n",
    "boolean_preprocessing_pipeline = Pipeline([\n",
    "    ('boolean_feature_transform', boolean_feature_preprocessor)\n",
    "])\n",
    "\n",
    "# Assuming train_df is your DataFrame\n",
    "# Apply transformations and create a new DataFrame with engineered features\n",
    "transformed_df = boolean_preprocessing_pipeline.fit_transform(train_df)\n",
    "transformed_df = pd.DataFrame(transformed_df, columns=[\n",
    "    'total_valid_phones', 'foreign_long_session', 'device_and_account_history', 'total_risk_flags', 'source_encoded'\n",
    "])\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(transformed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOULD BE MOVED TO DATAPREPROCESSING?\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "encoded_train_df = pd.get_dummies(train_df, columns=cat_df.columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOULD BE MOVED TO DATAPREPROCESSING?\n",
    "\n",
    "# Apply log transformation\n",
    "for column in ['velocity_6h', 'velocity_24h', 'zip_count_4w']:\n",
    "    train_df[column] = np.log1p(train_df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "   df['MISSING_FLAG_intended_balcon_amount'] = df['intended_balcon_amount'].isna().astype(int)\n",
    "   df['intended_balcon_amount'].fillna(-1, inplace=True)\n",
    "   df['MISSING_FLAG_prev_address_months_count'] = df['prev_address_months_count'].isna().astype(int)\n",
    "   df['prev_address_months_count'].fillna(-1, inplace=True)\n",
    "   df['MISSING_FLAG_current_address_months_count'] = df['current_address_months_count'].isna().astype(int)\n",
    "   df['current_address_months_count'].fillna(-1, inplace=True)\n",
    "   df['MISSING_FLAG_bank_months_count'] = df['bank_months_count'].isna().astype(int)\n",
    "   df['bank_months_count'].fillna(-1, inplace=True)\n",
    "   df['session_length_in_minutes'].fillna(-1, inplace=True)\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
