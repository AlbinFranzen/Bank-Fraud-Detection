{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "This jupyter notebook creates an sklearn pipeline for our data preprocessing. The steps that we need to follow are motivated by the EDA and are split into\n",
    "\n",
    "- Numerical features\n",
    "- High-cardinality categorical features\n",
    "- Low-cardinality categorical features\n",
    "- Boolean categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Binarizer\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data has 800000 rows and 32 columns.\n",
      "The testing data has 200000 rows and 32 columns.\n"
     ]
    }
   ],
   "source": [
    "# Data collection\n",
    "total_df = pd.read_csv('./Data/Base.csv')\n",
    "\n",
    "# Split the DataFrame into training and test sets using stratified sampling to maintain anomaly distribution\n",
    "train_df, test_df = train_test_split(total_df, test_size=0.2, stratify=total_df['fraud_bool'], random_state=42)\n",
    "\n",
    "# Validate the size of the data\n",
    "train_shape = train_df.shape\n",
    "test_shape = test_df.shape\n",
    "print(f\"The training data has {train_shape[0]} rows and {train_shape[1]} columns.\")\n",
    "print(f\"The testing data has {test_shape[0]} rows and {test_shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations for numerical features\n",
    "\n",
    "The numerical features will be transformed as follows\n",
    "\n",
    "Name_email_similarity:\n",
    "- Convert into 5 bins, 4 with range 0.24 and 1 with range 0.04 (0.96-1)\n",
    "- There are no outliers\n",
    "\n",
    "Days_since_request:\n",
    "- Convert into 3 bins [0->Q2, Q2->Q3+1.5* IQR, Q3+1.5*IQR+]\n",
    "\n",
    "Intended_balcon_amount\n",
    "- Replace with boolean feature if positive account\n",
    "\n",
    "Velocity_6h, _24h, 4w\n",
    "- Remove all\n",
    "\n",
    "Session_length_in_minutes\n",
    "- Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer for binning 'name_email_similarity'\n",
    "class NameEmailSimilarityBinner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.bin_edges = np.array([0.0, 0.24, 0.48, 0.72, 0.96, 1.0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.digitize(X.to_numpy().flatten(), bins=self.bin_edges, right=False).reshape(-1, 1)\n",
    "\n",
    "# Custom transformer for binning 'days_since_request'\n",
    "class DaysSinceRequestBinner(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        X_series = X.to_numpy().flatten()\n",
    "        self.Q2 = np.quantile(X_series, 0.5)\n",
    "        self.upper_fence = np.quantile(X_series, 0.75) + 1.5 * (np.quantile(X_series, 0.75) - np.quantile(X_series, 0.25))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        bin_edges = np.array([-np.inf, self.Q2, self.upper_fence, np.inf])\n",
    "        return np.digitize(X.to_numpy().flatten(), bins=bin_edges, right=False).reshape(-1, 1)\n",
    "\n",
    "# Pipeline to drop velocity columns and apply transformations while keeping other columns\n",
    "numerical_preprocessor = Pipeline([\n",
    "    ('drop_velocity_columns', FunctionTransformer(lambda X: X.drop(columns=[\"velocity_6h\", \"velocity_24h\", \"velocity_4w\"]))),\n",
    "    ('transformer', ColumnTransformer([\n",
    "        ('name_email_similarity_binned', NameEmailSimilarityBinner(), ['name_email_similarity']),\n",
    "        ('days_since_request_binned', DaysSinceRequestBinner(), ['days_since_request']),\n",
    "        ('intended_balcon_amount_positive', Binarizer(threshold=0.0), ['intended_balcon_amount'])\n",
    "    ], remainder='passthrough'))  # Keep other columns as they are\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations for high-cardinality categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations for low-cardinality categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformations for boolean categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For feature selection, we will retain features with strong associations to fraud and drop or combine weaker predictors. Since binary features generally donâ€™t require scaling, they can move directly into modeling, though combining weaker features may enhance their predictive utility. With this strategy, we can optimize the dataset for fraud prediction while reducing unnecessary complexity.\n",
    "\n",
    "### Summary of feature transformations for boolean features\n",
    "\n",
    "fraud_bool\n",
    "- Retain as it is the target variable\n",
    "\n",
    "email_is_free\n",
    "- Retain\n",
    "\n",
    "phone_home_valid and phone_mobile_valid\n",
    "- total_valid_phones which counts valid phone numbers (0, 1, or 2) by combining phone_home_valid and phone_mobile_valid.\n",
    "\n",
    "has_other_cards\n",
    "- Retain\n",
    "\n",
    "foreign_request and keep_alive_session\n",
    "- Combined feature foreign_long_session to check for long session times from foreign locations which might indicate unusual behavior, like accessing accounts from unfamiliar regions.\n",
    "\n",
    "source\n",
    "- One-hot encoding\n",
    "\n",
    "device_fraud_count and keep_alive_session\n",
    "- Combine to create a flag that captures both device history and session longevity, potentially signaling abnormal use patterns.\n",
    "\n",
    "total_risk_flags\n",
    "- A derived aggregated fraud risk score using high-risk indicators (email_is_free, foreign_request, has_other_cards, and keep_alive_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to create `total_valid_phones`\n",
    "class TotalValidPhonesTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X['phone_home_valid'] + X['phone_mobile_valid']).values.reshape(-1, 1)\n",
    "\n",
    "# Custom transformer to create `foreign_long_session`\n",
    "class ForeignLongSessionTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X['foreign_request'] & X['keep_alive_session']).astype(int).values.reshape(-1, 1)\n",
    "\n",
    "# Custom transformer to create `device_and_account_history`\n",
    "class DeviceAccountHistoryTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X['device_fraud_count'] * X['keep_alive_session']).values.reshape(-1, 1)\n",
    "\n",
    "# Custom transformer to create `total_risk_flags`\n",
    "class TotalRiskFlagsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.high_risk_features = ['email_is_free', 'foreign_request', 'has_other_cards', 'keep_alive_session']\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.high_risk_features].sum(axis=1).values.reshape(-1, 1)\n",
    "\n",
    "# Pipeline to create new features from Boolean data\n",
    "boolean_feature_preprocessor = ColumnTransformer([\n",
    "    ('total_valid_phones', TotalValidPhonesTransformer(), ['phone_home_valid', 'phone_mobile_valid']),\n",
    "    ('foreign_long_session', ForeignLongSessionTransformer(), ['foreign_request', 'keep_alive_session']),\n",
    "    ('device_and_account_history', DeviceAccountHistoryTransformer(), ['device_fraud_count', 'keep_alive_session']),\n",
    "    ('total_risk_flags', TotalRiskFlagsTransformer(), ['email_is_free', 'foreign_request', 'has_other_cards', 'keep_alive_session']),\n",
    "    ('source_encoded', 'passthrough', ['source'])  # Assuming 'source' is preprocessed with one-hot encoding externally\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Sample implementation for preprocessing pipeline\n",
    "boolean_preprocessing_pipeline = Pipeline([\n",
    "    ('boolean_feature_transform', boolean_feature_preprocessor)\n",
    "])\n",
    "\n",
    "# Assuming train_df is your DataFrame\n",
    "# Apply transformations and create a new DataFrame with engineered features\n",
    "transformed_df = boolean_preprocessing_pipeline.fit_transform(train_df)\n",
    "transformed_df = pd.DataFrame(transformed_df, columns=[\n",
    "    'total_valid_phones', 'foreign_long_session', 'device_and_account_history', 'total_risk_flags', 'source_encoded'\n",
    "])\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(transformed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOULD BE MOVED TO DATAPREPROCESSING?\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "encoded_train_df = pd.get_dummies(train_df, columns=cat_df.columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOULD BE MOVED TO DATAPREPROCESSING?\n",
    "\n",
    "# Apply log transformation\n",
    "for column in ['velocity_6h', 'velocity_24h', 'zip_count_4w']:\n",
    "    train_df[column] = np.log1p(train_df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "   df['MISSING_FLAG_intended_balcon_amount'] = df['intended_balcon_amount'].isna().astype(int)\n",
    "   df['intended_balcon_amount'].fillna(-1, inplace=True)\n",
    "   df['MISSING_FLAG_prev_address_months_count'] = df['prev_address_months_count'].isna().astype(int)\n",
    "   df['prev_address_months_count'].fillna(-1, inplace=True)\n",
    "   df['MISSING_FLAG_current_address_months_count'] = df['current_address_months_count'].isna().astype(int)\n",
    "   df['current_address_months_count'].fillna(-1, inplace=True)\n",
    "   df['MISSING_FLAG_bank_months_count'] = df['bank_months_count'].isna().astype(int)\n",
    "   df['bank_months_count'].fillna(-1, inplace=True)\n",
    "   df['session_length_in_minutes'].fillna(-1, inplace=True)\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
