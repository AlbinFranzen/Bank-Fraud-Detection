{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model devepment\n",
    "\n",
    "In this document we develop and compare different models for our model devepment. We have the following sections:\n",
    "\n",
    "1. Model creation\n",
    "2. Model evaluation\n",
    "3. Model implementation on test data\n",
    "\n",
    "Note that for model creation instead of running the code each time one can load the best model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from scipy.stats import uniform, randint\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "total_df = pd.read_csv('../Data/Base.csv')\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = total_df.drop(columns=['fraud_bool'])\n",
    "y = total_df['fraud_bool']\n",
    "\n",
    "# Split the data into training and test sets using stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "print(\"Data has been loaded\")\n",
    "\n",
    "# Apply the preprocessor to the training and test datasets\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Creation\n",
    "\n",
    "### 1a. Hyperparameter specification\n",
    "\n",
    "We decided to use Logistic Regression, Random Forests, Support Vector Classifier, KNN, Gradient Boosting, XGBoost, LightGBM and Naive Bayes. The following hyperparameters were what was decided to be best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline with placeholder classifier\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LogisticRegression())])\n",
    "\n",
    "# Define models and their hyperparameters\n",
    "models = {\n",
    "    # 'Logistic Regression': (\n",
    "    #     LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000),\n",
    "    #     {'classifier__C': uniform(0.01, 10)}\n",
    "    #  ), \n",
    "    # 'Random Forest': (\n",
    "    #     RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    #     {\n",
    "    #         'classifier__n_estimators': randint(50, 150),\n",
    "    #         'classifier__max_depth': randint(3, 10)\n",
    "    #     }\n",
    "    # ),\n",
    "    # 'SVC': (\n",
    "    #     SVC(class_weight='balanced', probability=True, random_state=42),\n",
    "    #     {\n",
    "    #         'classifier__C': uniform(0.01, 10),\n",
    "    #         'classifier__kernel': ['linear', 'rbf']\n",
    "    #     }\n",
    "    # ),\n",
    "    # 'KNN': (\n",
    "    #     KNeighborsClassifier(),\n",
    "    #     {'classifier__n_neighbors': randint(3, 10)}\n",
    "    # ),\n",
    "    # 'Gradient Boosting': (\n",
    "    #     GradientBoostingClassifier(random_state=42),\n",
    "    #     {\n",
    "    #         'classifier__n_estimators': randint(50, 150),\n",
    "    #         'classifier__learning_rate': uniform(0.01, 0.2)\n",
    "    #     }\n",
    "    # ),\n",
    "    'XGBoost': (\n",
    "        XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss', tree_method='gpu_hist'),\n",
    "        {\n",
    "            'classifier__n_estimators': randint(50, 150),\n",
    "            'classifier__learning_rate': uniform(0.01, 0.2),\n",
    "            'classifier__max_depth': randint(3, 10),\n",
    "            'classifier__gamma': uniform(0, 0.5),\n",
    "        }\n",
    "    ),\n",
    "    'LightGBM': (\n",
    "        LGBMClassifier(random_state=42, device='gpu'),\n",
    "        {\n",
    "            'classifier__n_estimators': randint(50, 150),\n",
    "            'classifier__learning_rate': uniform(0.01, 0.2),\n",
    "            'classifier__max_depth': randint(3, 10),\n",
    "            'classifier__num_leaves': randint(20, 50),\n",
    "        }\n",
    "    ),\n",
    "    # 'Naive Bayes': (\n",
    "    #     GaussianNB(),\n",
    "    #     {}  # No hyperparameters for Naive Bayes\n",
    "    # )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Hyperparameter searching \n",
    "\n",
    "The next step is to do the hyperparameter search and we decided to do 3 random searches per model to keep the time complexity low. This code takes a lot of time to run but is a sacrifice our computers are willing to take.\n",
    "\n",
    "### (WARNING: DON'T RUN CELL, LOAD SEARCH INSTEAD!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting RandomizedSearchCV for XGBoost (Version 1) ---\n",
      "\n",
      "Fitting the model XGBoost with 50 iterations and 5 cross-validation splits...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michael\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:23:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\Michael\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:23:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Finished RandomizedSearchCV for XGBoost in 334.26s ---\n",
      "\n",
      "Saving RandomizedSearchCV results for XGBoost as saved_models/XGBoost_v1_2024-11-20_00-23-10_334.26s.joblib...\n",
      "\n",
      "RandomizedSearchCV for XGBoost saved successfully.\n",
      "\n",
      "\n",
      "--- Starting RandomizedSearchCV for LightGBM (Version 2) ---\n",
      "\n",
      "Fitting the model LightGBM with 50 iterations and 5 cross-validation splits...\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michael\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:23:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 2307\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 46\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 4070 SUPER, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 22 dense feature groups (18.31 MB) transferred to GPU in 0.013352 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "--- Finished RandomizedSearchCV for LightGBM in 681.50s ---\n",
      "\n",
      "Saving RandomizedSearchCV results for LightGBM as saved_models/LightGBM_v2_2024-11-20_00-34-32_681.50s.joblib...\n",
      "\n",
      "RandomizedSearchCV for LightGBM saved successfully.\n",
      "\n",
      "\n",
      "--- All Models Processed ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create directory for saving models if it doesn't exist\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Number of random searches per model\n",
    "n_iter_per_model = 50  # Adjust as needed\n",
    "best_models = []\n",
    "\n",
    "# Dictionary to store all RandomizedSearchCV objects\n",
    "search_results = {}\n",
    "\n",
    "# Iterate through each model\n",
    "version_counter = 1\n",
    "for name, (model, params) in models.items():\n",
    "    print(f\"\\n--- Starting RandomizedSearchCV for {name} (Version {version_counter}) ---\\n\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "\n",
    "    # Create RandomizedSearchCV\n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline, \n",
    "        param_distributions=params,\n",
    "        n_iter=n_iter_per_model,\n",
    "        cv=stratified_cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        scoring='roc_auc',\n",
    "        verbose=3\n",
    "    )\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fit the model\n",
    "    try:\n",
    "        print(f\"Fitting the model {name} with {n_iter_per_model} iterations and {stratified_cv.get_n_splits()} cross-validation splits...\")\n",
    "        search.fit(X_train, y_train)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fitting {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_time_str = f\"{elapsed_time:.2f}s\"\n",
    "    print(f\"\\n--- Finished RandomizedSearchCV for {name} in {elapsed_time_str} ---\\n\")\n",
    "\n",
    "    # Store the search object in the dictionary\n",
    "    search_results[name] = search\n",
    "\n",
    "    # Get current date and time for naming\n",
    "    current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # Create a unique filename with version number, model name, date, and training time\n",
    "    filename = f\"saved_models/{name}_v{version_counter}_{current_datetime}_{elapsed_time_str}.joblib\"\n",
    "    \n",
    "    # Save each RandomizedSearchCV object immediately after training\n",
    "    print(f\"Saving RandomizedSearchCV results for {name} as {filename}...\\n\")\n",
    "    joblib.dump(search, filename)\n",
    "    print(f\"RandomizedSearchCV for {name} saved successfully.\\n\")\n",
    "\n",
    "    # Increment the version number for the next model\n",
    "    version_counter += 1\n",
    "\n",
    "print(\"\\n--- All Models Processed ---\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b.5 AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Search saving and loading\n",
    "\n",
    "Save search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined search results saved successfully as search_results.joblib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michael\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [00:34:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the saved joblib files\n",
    "saved_models_dir = 'saved_models'\n",
    "\n",
    "# Initialize the final search_results dictionary to store all models\n",
    "search_results = {}\n",
    "\n",
    "# Iterate over each joblib file in the directory\n",
    "for filename in os.listdir(saved_models_dir):\n",
    "    if filename.endswith(\".joblib\"):\n",
    "        # Construct the full path to the file\n",
    "        filepath = os.path.join(saved_models_dir, filename)\n",
    "\n",
    "        # Load the model information from the file\n",
    "        model_info = joblib.load(filepath)\n",
    "\n",
    "        # Use the filename (without extension) as the key in the search_results dictionary\n",
    "        model_key = filename.replace('.joblib', '')\n",
    "        \n",
    "        # Add the loaded model information to the search_results dictionary\n",
    "        search_results[model_key] = model_info\n",
    "\n",
    "# Save the combined search_results dictionary to a new joblib file\n",
    "combined_filename = 'search_results.joblib'\n",
    "joblib.dump(search_results, combined_filename)\n",
    "print(f\"Combined search results saved successfully as {combined_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = joblib.load(f\"search_results.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model evaluation\n",
    "\n",
    "### 2a. Random search evaluation\n",
    "\n",
    "Due to the biased data set we use the auc roc score to evaluate different models. We start by printing the best model of each model class for our search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Score (AUC)</th>\n",
       "      <th>Best Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM_v2_2024-11-20_00-34-32_681.50s</td>\n",
       "      <td>0.8903</td>\n",
       "      <td>{'classifier__learning_rate': 0.17043939615080...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost_v1_2024-11-20_00-23-10_334.26s</td>\n",
       "      <td>0.8903</td>\n",
       "      <td>{'classifier__gamma': 0.018443473677266398, 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost_v3_2024-11-20_00-08-29_19.72s</td>\n",
       "      <td>0.8882</td>\n",
       "      <td>{'classifier__learning_rate': 0.08490802376947...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM_v4_2024-11-20_00-08-53_24.18s</td>\n",
       "      <td>0.8854</td>\n",
       "      <td>{'classifier__learning_rate': 0.08490802376947...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting_v2_2024-11-20_00-08-09_585.78s</td>\n",
       "      <td>0.8847</td>\n",
       "      <td>{'classifier__learning_rate': 0.08490802376947...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression_search_results</td>\n",
       "      <td>0.8729</td>\n",
       "      <td>{'classifier__C': 9.51714306409916}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest_search_results</td>\n",
       "      <td>0.8691</td>\n",
       "      <td>{'classifier__max_depth': 9, 'classifier__n_es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes_v5_2024-11-20_00-09-05_11.48s</td>\n",
       "      <td>0.8371</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN_v1_2024-11-19_23-58-23_500.97s</td>\n",
       "      <td>0.6448</td>\n",
       "      <td>{'classifier__n_neighbors': 9}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Model Best Score (AUC)  \\\n",
       "2           LightGBM_v2_2024-11-20_00-34-32_681.50s           0.8903   \n",
       "7            XGBoost_v1_2024-11-20_00-23-10_334.26s           0.8903   \n",
       "8             XGBoost_v3_2024-11-20_00-08-29_19.72s           0.8882   \n",
       "3            LightGBM_v4_2024-11-20_00-08-53_24.18s           0.8854   \n",
       "0  Gradient Boosting_v2_2024-11-20_00-08-09_585.78s           0.8847   \n",
       "4                Logistic Regression_search_results           0.8729   \n",
       "6                      Random Forest_search_results           0.8691   \n",
       "5         Naive Bayes_v5_2024-11-20_00-09-05_11.48s           0.8371   \n",
       "1                KNN_v1_2024-11-19_23-58-23_500.97s           0.6448   \n",
       "\n",
       "                                     Best Parameters  \n",
       "2  {'classifier__learning_rate': 0.17043939615080...  \n",
       "7  {'classifier__gamma': 0.018443473677266398, 'c...  \n",
       "8  {'classifier__learning_rate': 0.08490802376947...  \n",
       "3  {'classifier__learning_rate': 0.08490802376947...  \n",
       "0  {'classifier__learning_rate': 0.08490802376947...  \n",
       "4                {'classifier__C': 9.51714306409916}  \n",
       "6  {'classifier__max_depth': 9, 'classifier__n_es...  \n",
       "5                                                 {}  \n",
       "1                     {'classifier__n_neighbors': 9}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a list for storing model information\n",
    "results_summary = []\n",
    "\n",
    "# Collect the best model, score, and parameters\n",
    "for model_name, search in search_results.items():\n",
    "    best_score = search.best_score_\n",
    "    best_params = search.best_params_\n",
    "    results_summary.append({\n",
    "        'Model': model_name,\n",
    "        'Best Score (AUC)': f\"{best_score:.4f}\",\n",
    "        'Best Parameters': best_params\n",
    "    })\n",
    "\n",
    "# Convert to a DataFrame and sort by AUC score\n",
    "results_df = pd.DataFrame(results_summary).sort_values(by='Best Score (AUC)', ascending=False)\n",
    "\n",
    "# Display the DataFrame in Jupyter\n",
    "from IPython.display import display\n",
    "\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best performing model on the trainig data is \"TODO\" with auc_roc score being \"TODO\". Now we will evaluate this model closer in the following section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Best model evaluation\n",
    "\n",
    "We begin the evaluation of the best model by extracting it from search_results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Name: Logistic Regression\n",
      "Best Model Score (AUC): 0.8729\n",
      "Best Model Parameters: {'classifier__C': 3.7554011884736247}\n",
      "Best Classifier Object: LogisticRegression(C=3.7554011884736247, class_weight='balanced', max_iter=1000,\n",
      "                   random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to track the best model\n",
    "best_model_name = None\n",
    "best_model_score = -float('inf')\n",
    "best_model_params = None\n",
    "best_model_object = None\n",
    "best_classifier = None\n",
    "\n",
    "# Iterate through the search results to find the best model\n",
    "for model_name, search in search_results.items():\n",
    "    if search.best_score_ > best_model_score:\n",
    "        best_model_name = model_name\n",
    "        best_model_score = search.best_score_\n",
    "        best_model_params = search.best_params_\n",
    "        best_model_object = search.best_estimator_\n",
    "\n",
    "        # Extract the classifier from the pipeline\n",
    "        best_classifier = best_model_object.named_steps['classifier']\n",
    "\n",
    "# Print the best model details\n",
    "print(f\"Best Model Name: {best_model_name}\")\n",
    "print(f\"Best Model Score (AUC): {best_model_score:.4f}\")\n",
    "print(f\"Best Model Parameters: {best_model_params}\")\n",
    "\n",
    "# Print the best classifier object\n",
    "print(f\"Best Classifier Object: {best_classifier}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we evaluate the model on the training data. This gives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test data evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "y_pred = best_classifier.predict(X_test)\n",
    "y_pred_proba = best_classifier.predict_proba(X_test)[:, 1] if hasattr(best_model_name.named_steps['classifier'], 'predict_proba') else y_pred\n",
    "\n",
    "# Print the best model and its parameters\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Best Cross-Validation AUC Score: {best_score:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculate and print AUC score on the test set\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"AUC Score on Test Set: {auc_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
