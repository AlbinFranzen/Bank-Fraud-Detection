{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model devepment\n",
    "\n",
    "In this document we develop and compare different models for our model devepment. We have the following sections:\n",
    "\n",
    "1. Model creation\n",
    "2. Model evaluation\n",
    "3. Model implementation on test data\n",
    "\n",
    "Note that for model creation instead of running the code each time one can load the best model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import preprocessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, recall_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from scipy.stats import uniform, randint\n",
    "import joblib\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "total_df = pd.read_csv('../Data/Base.csv')\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = total_df.drop(columns=['fraud_bool'])\n",
    "y = total_df['fraud_bool']\n",
    "\n",
    "# Split the data into training and test sets using stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "print(\"Data has been loaded\")\n",
    "\n",
    "# Apply the preprocessor to the training and test datasets\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Creation\n",
    "\n",
    "### 1a. Hyperparameter specification\n",
    "\n",
    "We will use the following hyperparameters for our models. They are based on common hyperparameters used for the respective models with ranges that are commonly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline with placeholder classifier\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LogisticRegression())])\n",
    "\n",
    "# Define models and hyperparameters\n",
    "models = {\n",
    "    'Logistic Regression (lbfgs)': (\n",
    "        LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000),\n",
    "        {\n",
    "            'classifier__C': uniform(0.01, 10),  # Regularization strength\n",
    "            'classifier__solver': ['lbfgs'],  # Only lbfgs solver\n",
    "            'classifier__penalty': ['l2', 'none'],  # Supported penalties\n",
    "        }\n",
    "    ),\n",
    "    # 'Logistic Regression (liblinear)': (\n",
    "    #     LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000),\n",
    "    #     {\n",
    "    #         'classifier__C': uniform(0.01, 10),  # Regularization strength\n",
    "    #         'classifier__solver': ['liblinear'],  # Only liblinear solver\n",
    "    #         'classifier__penalty': ['l1', 'l2'],  # Supported penalties\n",
    "    #     }\n",
    "    # ),\n",
    "    # 'Logistic Regression (saga)': (\n",
    "    #     LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000),\n",
    "    #     {\n",
    "    #         'classifier__C': uniform(0.01, 10),  # Regularization strength\n",
    "    #         'classifier__solver': ['saga'],  # Only saga solver\n",
    "    #         'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],  # All supported penalties\n",
    "    #         'classifier__l1_ratio': uniform(0, 1),  # ElasticNet mixing parameter (only with saga and elasticnet)\n",
    "    #     }\n",
    "    # ),\n",
    "    # 'Logistic Regression (default)': (\n",
    "    #     LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000),\n",
    "    #     {\n",
    "    #         'classifier__C': uniform(0.01, 10),  # Regularization strength\n",
    "    #     }\n",
    "    # ),\n",
    "    # 'Random Forest': (\n",
    "    #     RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    #     {\n",
    "    #         'classifier__n_estimators': randint(100, 300),  # Number of trees\n",
    "    #         'classifier__max_depth': randint(5, 20),  # Max tree depth\n",
    "    #         'classifier__min_samples_split': randint(2, 10),  # Min samples to split\n",
    "    #         'classifier__min_samples_leaf': randint(1, 5),  # Min samples per leaf\n",
    "    #         'classifier__class_weight': ['balanced', {0: 1, 1: 10}],  # Explicit weights\n",
    "    #     }\n",
    "    # ),\n",
    "    # 'Gradient Boosting': (\n",
    "    #     GradientBoostingClassifier(random_state=42),\n",
    "    #     {\n",
    "    #         'classifier__n_estimators': randint(50, 300),  # Number of boosting stages\n",
    "    #         'classifier__learning_rate': uniform(0.01, 0.2),  # Shrinkage rate\n",
    "    #         'classifier__max_depth': randint(3, 15),  # Max tree depth\n",
    "    #         'classifier__min_samples_split': randint(2, 10),  # Min samples to split\n",
    "    #         'classifier__min_samples_leaf': randint(1, 5),  # Min samples per leaf\n",
    "    #     }\n",
    "    # ),\n",
    "    'XGBoost': (\n",
    "        XGBClassifier(random_state=42, eval_metric='logloss', device='cuda', tree_method='hist'),\n",
    "        {\n",
    "            'classifier__n_estimators': randint(50, 300),  # Number of boosting stages\n",
    "            'classifier__learning_rate': uniform(0.01, 0.2),  # Learning rate\n",
    "            'classifier__max_depth': randint(3, 15),  # Maximum tree depth\n",
    "            'classifier__gamma': uniform(0, 0.5),  # Min split loss\n",
    "            'classifier__scale_pos_weight': [1, 5, 10, 20],  # Reweighting for imbalanced data\n",
    "        }\n",
    "    ),\n",
    "    'LightGBM': (\n",
    "        LGBMClassifier(random_state=42),\n",
    "        {\n",
    "            'classifier__n_estimators': randint(50, 300),  # Number of boosting stages\n",
    "            'classifier__learning_rate': uniform(0.01, 0.2),  # Learning rate\n",
    "            'classifier__max_depth': randint(3, 15),  # Maximum tree depth\n",
    "            'classifier__num_leaves': randint(20, 50),  # Number of leaves\n",
    "            # 'classifier__class_weight': ['balanced', {0: 1, 1: 10}],  # Explicit weights\n",
    "        }\n",
    "    ),\n",
    "    # 'Naive Bayes': (\n",
    "    #     GaussianNB(),\n",
    "    #     {}  # No hyperparameters for Naive Bayes\n",
    "    # ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Exploratory Hyperparameter Search \n",
    "\n",
    "To find the best hyperparameters for each model we will use RandomizedSearchCV because it is more efficient than GridSearchCV for the exploration of the hyperparameter space.\n",
    "\n",
    "Note: Running the following code is not recommended. It is just for reference. Running it will train the models, and we have provided our training mnodels in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamacore/FINAL/Bank-Fraud-Detection/bank/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting RandomizedSearchCV for Logistic Regression (lbfgs) ---\n",
      "\n",
      "Model Logistic Regression (lbfgs) already trained. Skipping...\n",
      "\n",
      "--- Starting RandomizedSearchCV for XGBoost ---\n",
      "\n",
      "Model XGBoost already trained. Skipping...\n",
      "\n",
      "--- Starting RandomizedSearchCV for LightGBM ---\n",
      "\n",
      "Fitting the model LightGBM with 5 iterations and 5 cross-validation splits...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[LightGBM] [Info] Number of positive: 7059, number of negative: 632941\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496074\n",
      "[LightGBM] [Info] Start training from score -4.496074\n",
      "[CV 1/5] END classifier__learning_rate=0.0849080237694725, classifier__max_depth=13, classifier__n_estimators=121, classifier__num_leaves=48;, score=0.494 total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 7059, number of negative: 632941\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496074\n",
      "[LightGBM] [Info] Start training from score -4.496074\n",
      "[CV 2/5] END classifier__learning_rate=0.0849080237694725, classifier__max_depth=13, classifier__n_estimators=121, classifier__num_leaves=48;, score=0.505 total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2307\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[CV 3/5] END classifier__learning_rate=0.0849080237694725, classifier__max_depth=13, classifier__n_estimators=121, classifier__num_leaves=48;, score=0.505 total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[CV 4/5] END classifier__learning_rate=0.0849080237694725, classifier__max_depth=13, classifier__n_estimators=121, classifier__num_leaves=48;, score=0.499 total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024964 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[CV 5/5] END classifier__learning_rate=0.0849080237694725, classifier__max_depth=13, classifier__n_estimators=121, classifier__num_leaves=48;, score=0.522 total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 7059, number of negative: 632941\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496074\n",
      "[LightGBM] [Info] Start training from score -4.496074\n",
      "[CV 1/5] END classifier__learning_rate=0.1293700315892974, classifier__max_depth=12, classifier__n_estimators=260, classifier__num_leaves=42;, score=0.481 total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 7059, number of negative: 632941\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496074\n",
      "[LightGBM] [Info] Start training from score -4.496074\n",
      "[CV 2/5] END classifier__learning_rate=0.1293700315892974, classifier__max_depth=12, classifier__n_estimators=260, classifier__num_leaves=42;, score=0.465 total time=   6.6s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2307\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[CV 3/5] END classifier__learning_rate=0.1293700315892974, classifier__max_depth=12, classifier__n_estimators=260, classifier__num_leaves=42;, score=0.477 total time=   6.3s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025596 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[CV 4/5] END classifier__learning_rate=0.1293700315892974, classifier__max_depth=12, classifier__n_estimators=260, classifier__num_leaves=42;, score=0.469 total time=   6.5s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[CV 5/5] END classifier__learning_rate=0.1293700315892974, classifier__max_depth=12, classifier__n_estimators=260, classifier__num_leaves=42;, score=0.482 total time=   6.4s\n",
      "[LightGBM] [Info] Number of positive: 7059, number of negative: 632941\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496074\n",
      "[LightGBM] [Info] Start training from score -4.496074\n",
      "[CV 1/5] END classifier__learning_rate=0.021616722433639893, classifier__max_depth=10, classifier__n_estimators=166, classifier__num_leaves=23;, score=0.493 total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 7059, number of negative: 632941\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024482 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496074\n",
      "[LightGBM] [Info] Start training from score -4.496074\n",
      "[CV 2/5] END classifier__learning_rate=0.021616722433639893, classifier__max_depth=10, classifier__n_estimators=166, classifier__num_leaves=23;, score=0.509 total time=   5.8s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2307\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[CV 3/5] END classifier__learning_rate=0.021616722433639893, classifier__max_depth=10, classifier__n_estimators=166, classifier__num_leaves=23;, score=0.512 total time=   5.7s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[CV 4/5] END classifier__learning_rate=0.021616722433639893, classifier__max_depth=10, classifier__n_estimators=166, classifier__num_leaves=23;, score=0.504 total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[CV 5/5] END classifier__learning_rate=0.021616722433639893, classifier__max_depth=10, classifier__n_estimators=166, classifier__num_leaves=23;, score=0.522 total time=   5.6s\n",
      "[LightGBM] [Info] Number of positive: 7059, number of negative: 632941\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496074\n",
      "[LightGBM] [Info] Start training from score -4.496074\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/5] END classifier__learning_rate=0.038573363584388155, classifier__max_depth=5, classifier__n_estimators=199, classifier__num_leaves=40;, score=0.507 total time=   5.9s\n",
      "[LightGBM] [Info] Number of positive: 7059, number of negative: 632941\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496074\n",
      "[LightGBM] [Info] Start training from score -4.496074\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 2/5] END classifier__learning_rate=0.038573363584388155, classifier__max_depth=5, classifier__n_estimators=199, classifier__num_leaves=40;, score=0.523 total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026366 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2307\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/5] END classifier__learning_rate=0.038573363584388155, classifier__max_depth=5, classifier__n_estimators=199, classifier__num_leaves=40;, score=0.522 total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 4/5] END classifier__learning_rate=0.038573363584388155, classifier__max_depth=5, classifier__n_estimators=199, classifier__num_leaves=40;, score=0.516 total time=   6.2s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 5/5] END classifier__learning_rate=0.038573363584388155, classifier__max_depth=5, classifier__n_estimators=199, classifier__num_leaves=40;, score=0.530 total time=   6.0s\n",
      "[LightGBM] [Info] Number of positive: 7059, number of negative: 632941\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496074\n",
      "[LightGBM] [Info] Start training from score -4.496074\n",
      "[CV 1/5] END classifier__learning_rate=0.20398197043239888, classifier__max_depth=14, classifier__n_estimators=207, classifier__num_leaves=25;, score=0.480 total time=   5.2s\n",
      "[LightGBM] [Info] Number of positive: 7059, number of negative: 632941\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496074\n",
      "[LightGBM] [Info] Start training from score -4.496074\n",
      "[CV 2/5] END classifier__learning_rate=0.20398197043239888, classifier__max_depth=14, classifier__n_estimators=207, classifier__num_leaves=25;, score=0.485 total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2307\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[CV 3/5] END classifier__learning_rate=0.20398197043239888, classifier__max_depth=14, classifier__n_estimators=207, classifier__num_leaves=25;, score=0.477 total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[CV 4/5] END classifier__learning_rate=0.20398197043239888, classifier__max_depth=14, classifier__n_estimators=207, classifier__num_leaves=25;, score=0.475 total time=   5.3s\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[CV 5/5] END classifier__learning_rate=0.20398197043239888, classifier__max_depth=14, classifier__n_estimators=207, classifier__num_leaves=25;, score=0.486 total time=   5.5s\n",
      "[LightGBM] [Info] Number of positive: 8823, number of negative: 791177\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2307\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011029 -> initscore=-4.496160\n",
      "[LightGBM] [Info] Start training from score -4.496160\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "--- Finished RandomizedSearchCV for LightGBM in 150.89s ---\n",
      "\n",
      "Saving RandomizedSearchCV results for LightGBM as saved_models/LightGBM_2024-12-09_02-24-26_150.89s.joblib...\n",
      "\n",
      "RandomizedSearchCV for LightGBM saved successfully.\n",
      "\n",
      "\n",
      "--- All Models Processed ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm # Progress bar\n",
    "from glob import glob # File search\n",
    "from sklearn.metrics import roc_curve\n",
    "import numpy as np\n",
    "\n",
    "# Custom recall scorer at a target FPR threshold\n",
    "def recall_at_fpr(y_true, y_scores, target_fpr=0.05):\n",
    "    # Ensure y_true contains at least one positive and one negative sample\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return np.nan  # Return NaN if there's no variability in the target\n",
    "\n",
    "    # Ensure y_scores is valid\n",
    "    if np.any(np.isnan(y_scores)) or np.any(np.isinf(y_scores)):\n",
    "        return np.nan  # Return NaN for invalid scores\n",
    "\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "\n",
    "    # If no threshold meets the target FPR, return 0.0\n",
    "    if len(np.where(fpr <= target_fpr)[0]) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Get the highest threshold index where FPR is <= target FPR\n",
    "    threshold_idx = np.where(fpr <= target_fpr)[0][-1]\n",
    "    return tpr[threshold_idx]\n",
    "\n",
    "# Wrapper function to pass the target FPR\n",
    "def recall_at_fpr_scorer(y_true, y_scores):\n",
    "    return recall_at_fpr(y_true, y_scores, target_fpr=0.05)\n",
    "\n",
    "# Make a scorer for recall @ 5% FPR\n",
    "from sklearn.metrics import make_scorer\n",
    "recall_5_fpr_scorer = make_scorer(recall_at_fpr_scorer, greater_is_better=True, needs_proba=True)\n",
    "\n",
    "# Create directory for saving models if it doesn't exist\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Number of random searches per model\n",
    "n_iter_per_model = 5\n",
    "best_models = []\n",
    "\n",
    "# Dictionary to store all RandomizedSearchCV objects\n",
    "search_results = {}\n",
    "\n",
    "# Iterate through each model\n",
    "for name, (model, params) in models.items():\n",
    "    print(f\"\\n--- Starting RandomizedSearchCV for {name} ---\\n\")\n",
    "    \n",
    "    # Check if the model has already been trained and skip if it exists\n",
    "    saved_files = glob(f\"saved_models/{name}_*.joblib\")\n",
    "    if saved_files:\n",
    "        print(f\"Model {name} already trained. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "\n",
    "    # Create RandomizedSearchCV\n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline, \n",
    "        param_distributions=params,\n",
    "        n_iter=n_iter_per_model,\n",
    "        cv=stratified_cv,\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "        scoring=recall_5_fpr_scorer,\n",
    "        verbose=3\n",
    "    )\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fit the model\n",
    "    try:\n",
    "        print(f\"Fitting the model {name} with {n_iter_per_model} iterations and {stratified_cv.get_n_splits()} cross-validation splits...\")\n",
    "        search.fit(X_train, y_train)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fitting {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_time_str = f\"{elapsed_time:.2f}s\"\n",
    "    print(f\"\\n--- Finished RandomizedSearchCV for {name} in {elapsed_time_str} ---\\n\")\n",
    "\n",
    "    # Store the search object in the dictionary\n",
    "    search_results[name] = search\n",
    "\n",
    "    # Get current date and time for naming\n",
    "    current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # Create a unique filename with version number, model name, date, and training time\n",
    "    filename = f\"saved_models/{name}_{current_datetime}_{elapsed_time_str}.joblib\"\n",
    "    \n",
    "    # Save each RandomizedSearchCV object immediately after training\n",
    "    print(f\"Saving RandomizedSearchCV results for {name} as {filename}...\\n\")\n",
    "    joblib.dump(search, filename)\n",
    "    print(f\"RandomizedSearchCV for {name} saved successfully.\\n\")\n",
    "\n",
    "print(\"\\n--- All Models Processed ---\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Search saving and loading\n",
    "\n",
    "Save search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamacore/FINAL/Bank-Fraud-Detection/bank/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'recall_at_fpr_scorer' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(saved_models_dir, filename)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the model information from the file\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model_info \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Use the filename (without extension) as the key in the search_results dictionary\u001b[39;00m\n\u001b[1;32m     19\u001b[0m model_key \u001b[38;5;241m=\u001b[39m filename\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/FINAL/Bank-Fraud-Detection/bank/lib/python3.10/site-packages/joblib/numpy_pickle.py:658\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m--> 658\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/FINAL/Bank-Fraud-Detection/bank/lib/python3.10/site-packages/joblib/numpy_pickle.py:577\u001b[0m, in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[1;32m    579\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease regenerate this pickle file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m                       \u001b[38;5;241m%\u001b[39m filename,\n\u001b[1;32m    583\u001b[0m                       \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1213\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:1538\u001b[0m, in \u001b[0;36m_Unpickler.load_stack_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTACK_GLOBAL requires str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:1582\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28m__import__\u001b[39m(module, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m-> 1582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_getattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:331\u001b[0m, in \u001b[0;36m_getattribute\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    329\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, subpath)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt get attribute \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m                              \u001b[38;5;241m.\u001b[39mformat(name, obj)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, parent\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'recall_at_fpr_scorer' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "# Define the directory containing the saved joblib files\n",
    "saved_models_dir = 'saved_models'\n",
    "\n",
    "# Initialize the final search_results dictionary to store all models\n",
    "search_results = {}\n",
    "\n",
    "# Iterate over each joblib file in the directory\n",
    "for filename in os.listdir(saved_models_dir):\n",
    "    if filename.endswith(\".joblib\"):\n",
    "        # Construct the full path to the file\n",
    "        filepath = os.path.join(saved_models_dir, filename)\n",
    "\n",
    "        # Load the model information from the file\n",
    "        model_info = joblib.load(filepath)\n",
    "\n",
    "        # Use the filename (without extension) as the key in the search_results dictionary\n",
    "        model_key = filename.replace('.joblib', '')\n",
    "        \n",
    "        # Add the loaded model information to the search_results dictionary\n",
    "        search_results[model_key] = model_info\n",
    "\n",
    "# Save the combined search_results dictionary to a new joblib file\n",
    "combined_filename = 'aaasearch_results.joblib'\n",
    "joblib.dump(search_results, combined_filename)\n",
    "print(f\"Combined search results saved successfully as {combined_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "# Define the directory containing the saved joblib files\n",
    "saved_models_dir = 'saved_models'\n",
    "\n",
    "# Initialize the final search_results dictionary to store all models\n",
    "search_results = {}\n",
    "\n",
    "# Iterate over each joblib file in the directory\n",
    "for filename in os.listdir(saved_models_dir):\n",
    "    if filename.endswith(\".joblib\"):\n",
    "        # Construct the full path to the file\n",
    "        filepath = os.path.join(saved_models_dir, filename)\n",
    "\n",
    "        # Load the model information from the file\n",
    "        model_info = joblib.load(filepath)\n",
    "\n",
    "        # Use the filename (without extension) as the key in the search_results dictionary\n",
    "        model_key = filename.replace('.joblib', '')\n",
    "        \n",
    "        # Add the loaded model information to the search_results dictionary\n",
    "        search_results[model_key] = model_info\n",
    "\n",
    "# Save the combined search_results dictionary to a new joblib file\n",
    "combined_filename = 'aaasearch_results.joblib'\n",
    "joblib.dump(search_results, combined_filename)\n",
    "print(f\"Combined search results saved successfully as {combined_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = joblib.load(f\"search_results.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model evaluation\n",
    "\n",
    "### 2a. Random search evaluation\n",
    "\n",
    "Due to the biased data set we use the auc roc score to evaluate different models. We start by printing the best model of each model class for our search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Score (AUC)</th>\n",
       "      <th>TPR @ 5% FPR</th>\n",
       "      <th>Threshold @ 5% FPR</th>\n",
       "      <th>Best Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBM_2024-12-09_02-24-26_150.89s</td>\n",
       "      <td>0.8878</td>\n",
       "      <td>53.54%</td>\n",
       "      <td>0.04</td>\n",
       "      <td>{'classifier__learning_rate': 0.03857336358438...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting_2024-12-09_01-48-51_20030.52s</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>52.99%</td>\n",
       "      <td>0.04</td>\n",
       "      <td>{'classifier__learning_rate': 0.04120372808848...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost_2024-12-09_02-19-15_209.47s</td>\n",
       "      <td>0.8809</td>\n",
       "      <td>51.45%</td>\n",
       "      <td>0.22</td>\n",
       "      <td>{'classifier__gamma': 0.2984250789732435, 'cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest_2024-12-08_20-15-00_3690.86s</td>\n",
       "      <td>0.8759</td>\n",
       "      <td>51.81%</td>\n",
       "      <td>0.16</td>\n",
       "      <td>{'classifier__class_weight': {0: 1, 1: 10}, 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (lbfgs)_v1_2024-12-08_19-0...</td>\n",
       "      <td>0.8740</td>\n",
       "      <td>48.73%</td>\n",
       "      <td>0.81</td>\n",
       "      <td>{'classifier__C': 4.468327528535911, 'classifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression (default)_2024-12-08_19-11...</td>\n",
       "      <td>0.8740</td>\n",
       "      <td>48.78%</td>\n",
       "      <td>0.81</td>\n",
       "      <td>{'classifier__C': 9.51714306409916}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes_2024-12-09_01-53-13_19.49s</td>\n",
       "      <td>0.8416</td>\n",
       "      <td>42.79%</td>\n",
       "      <td>0.84</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model Best Score (AUC)  \\\n",
       "6               LightGBM_2024-12-09_02-24-26_150.89s           0.8878   \n",
       "4    Gradient Boosting_2024-12-09_01-48-51_20030.52s           0.8874   \n",
       "3                XGBoost_2024-12-09_02-19-15_209.47s           0.8809   \n",
       "5         Random Forest_2024-12-08_20-15-00_3690.86s           0.8759   \n",
       "0  Logistic Regression (lbfgs)_v1_2024-12-08_19-0...           0.8740   \n",
       "2  Logistic Regression (default)_2024-12-08_19-11...           0.8740   \n",
       "1             Naive Bayes_2024-12-09_01-53-13_19.49s           0.8416   \n",
       "\n",
       "  TPR @ 5% FPR Threshold @ 5% FPR  \\\n",
       "6       53.54%               0.04   \n",
       "4       52.99%               0.04   \n",
       "3       51.45%               0.22   \n",
       "5       51.81%               0.16   \n",
       "0       48.73%               0.81   \n",
       "2       48.78%               0.81   \n",
       "1       42.79%               0.84   \n",
       "\n",
       "                                     Best Parameters  \n",
       "6  {'classifier__learning_rate': 0.03857336358438...  \n",
       "4  {'classifier__learning_rate': 0.04120372808848...  \n",
       "3  {'classifier__gamma': 0.2984250789732435, 'cla...  \n",
       "5  {'classifier__class_weight': {0: 1, 1: 10}, 'c...  \n",
       "0  {'classifier__C': 4.468327528535911, 'classifi...  \n",
       "2                {'classifier__C': 9.51714306409916}  \n",
       "1                                                 {}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Placeholder for storing extended metrics\n",
    "results_summary = []\n",
    "\n",
    "# Collect extended metrics for each trained model\n",
    "for model_name, search in search_results.items():\n",
    "    # Retrieve the best estimator from the search object\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    # Predict probabilities on the test data (assuming test data exists as X_test, y_test)\n",
    "    y_scores = best_model.predict_proba(X_test)[:, 1]  # Probability for positive class\n",
    "    auc_score = roc_auc_score(y_test, y_scores)\n",
    "\n",
    "    # Calculate TPR at 5% FPR\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "    target_fpr = 0.05\n",
    "\n",
    "    # Find the TPR and threshold at target FPR\n",
    "    if len(np.where(fpr <= target_fpr)[0]) > 0:\n",
    "        threshold_idx = np.where(fpr <= target_fpr)[0][-1]\n",
    "        tpr_at_fpr = tpr[threshold_idx]\n",
    "        threshold_at_fpr = thresholds[threshold_idx]\n",
    "    else:\n",
    "        tpr_at_fpr = 0.0\n",
    "        threshold_at_fpr = None\n",
    "\n",
    "    # Append model results\n",
    "    results_summary.append({\n",
    "        'Model': model_name,\n",
    "        'Best Score (AUC)': f\"{auc_score:.4f}\",\n",
    "        'TPR @ 5% FPR': f\"{tpr_at_fpr * 100:.2f}%\",\n",
    "        'Threshold @ 5% FPR': f\"{threshold_at_fpr:.2f}\" if threshold_at_fpr is not None else \"N/A\",\n",
    "        'Best Parameters': search.best_params_\n",
    "    })\n",
    "\n",
    "# Convert to a DataFrame and sort by AUC score\n",
    "results_df = pd.DataFrame(results_summary).sort_values(by='Best Score (AUC)', ascending=False)\n",
    "\n",
    "# Display the DataFrame in Jupyter\n",
    "from IPython.display import display\n",
    "\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Best model evaluation\n",
    "\n",
    "We begin the evaluation of the best model by extracting it from search_results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Name: Gradient Boosting_2024-12-09_01-48-51_20030.52s\n",
      "Best Model Score (AUC): 0.5196\n",
      "Best Model Parameters: {'classifier__learning_rate': 0.0412037280884873, 'classifier__max_depth': 5, 'classifier__min_samples_leaf': 3, 'classifier__min_samples_split': 4, 'classifier__n_estimators': 252}\n",
      "Best Classifier Object: GradientBoostingClassifier(learning_rate=0.0412037280884873, max_depth=5,\n",
      "                           min_samples_leaf=3, min_samples_split=4,\n",
      "                           n_estimators=252, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to track the best model\n",
    "best_model_name = None\n",
    "best_model_score = -float('inf')\n",
    "best_model_params = None\n",
    "best_model_object = None\n",
    "best_classifier = None\n",
    "\n",
    "# Iterate through the search results to find the best model\n",
    "for model_name, search in search_results.items():\n",
    "    if search.best_score_ > best_model_score:\n",
    "        best_model_name = model_name\n",
    "        best_model_score = search.best_score_\n",
    "        best_model_params = search.best_params_\n",
    "        best_model_object = search.best_estimator_\n",
    "\n",
    "        # Extract the classifier from the pipeline\n",
    "        best_classifier = best_model_object.named_steps['classifier']\n",
    "\n",
    "# Print the best model details\n",
    "print(f\"Best Model Name: {best_model_name}\")\n",
    "print(f\"Best Model Score (AUC): {best_model_score:.4f}\")\n",
    "print(f\"Best Model Parameters: {best_model_params}\")\n",
    "\n",
    "# Print the best classifier object\n",
    "print(f\"Best Classifier Object: {best_classifier}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continued Model Development\n",
    "\n",
    "Given that LightGBM and XGBoost are the top performing models, we will continue to develop these models further. We will start by tuning the hyperparameters of the models using Optuna, which is a library that applies Bayesian optimization to hyperparameter tuning. They adaptively explore the search space, focusing on more promising hyperparameter sets, which theoretically makes them more effective than RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    # \"LightGBM\": {\n",
    "    #     \"model\": \"lgb.LGBMClassifier\",\n",
    "    #     \"param_distributions\": {\n",
    "    #         \"classifier__num_leaves\": [31, 63, 127],\n",
    "    #         \"classifier__max_depth\": [-1, 5, 10, 20],\n",
    "    #         \"classifier__learning_rate\": uniform(loc=0.01, scale=0.09),\n",
    "    #         \"classifier__n_estimators\": randint(low=100, high=1000),\n",
    "    #         \"classifier__min_child_weight\": [1, 10, 100],\n",
    "    #         \"classifier__subsample\": uniform(loc=0.5, scale=0.5),  # Fix here\n",
    "    #         \"classifier__colsample_bytree\": uniform(loc=0.5, scale=0.5),\n",
    "    #         \"classifier__scale_pos_weight\": [10, 50, 100, 500],\n",
    "    #         \"classifier__boosting_type\": [\"gbdt\", \"dart\"],\n",
    "    #         \"classifier__objective\": [\"binary\"],\n",
    "    #         \"classifier__metric\": [\"auc\"]\n",
    "    #     }\n",
    "    # },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(random_state=42, eval_metric=\"aucpr\", device=\"cuda\", tree_method=\"hist\"),\n",
    "        \"param_distributions\": {\n",
    "            \"classifier__max_depth\": randint(low=3, high=10),\n",
    "            \"classifier__learning_rate\": uniform(loc=0.01, scale=0.09),\n",
    "            \"classifier__n_estimators\": randint(low=100, high=1000),\n",
    "            \"classifier__subsample\": uniform(loc=0.5, scale=0.5),  # Fix here\n",
    "            \"classifier__colsample_bytree\": uniform(loc=0.5, scale=0.5),\n",
    "            \"classifier__min_child_weight\": [1, 5, 10, 50],\n",
    "            \"classifier__scale_pos_weight\": [10, 50, 100, 500],\n",
    "            \"classifier__gamma\": uniform(loc=0, scale=5),\n",
    "            \"classifier__reg_alpha\": uniform(loc=0, scale=1),\n",
    "            \"classifier__reg_lambda\": uniform(loc=0, scale=1),\n",
    "            \"classifier__objective\": [\"binary:logistic\"],\n",
    "            \"classifier__eval_metric\": [\"aucpr\"],\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 7059, number of negative: 632941\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496074\n",
      "[LightGBM] [Info] Start training from score -4.496074\n",
      "[LightGBM] [Info] Number of positive: 7059, number of negative: 632941\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011030 -> initscore=-4.496074\n",
      "[LightGBM] [Info] Start training from score -4.496074\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2307\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n",
      "[LightGBM] [Info] Number of positive: 7058, number of negative: 632942\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2318\n",
      "[LightGBM] [Info] Number of data points in the train set: 640000, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011028 -> initscore=-4.496217\n",
      "[LightGBM] [Info] Start training from score -4.496217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:37:14,016] A new study created in memory with name: study_XGBoost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Optuna optimization for XGBoost ---\n",
      "\n",
      "Created new study for XGBoost.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:37:53,922] Trial 0 finished with value: 0.4790884096792635 and parameters: {'classifier__max_depth': 9, 'classifier__learning_rate': 0.09000188442707203, 'classifier__n_estimators': 224, 'classifier__subsample': 0.8115890408643198, 'classifier__colsample_bytree': 0.8794835538963024, 'classifier__min_child_weight': 50, 'classifier__scale_pos_weight': 500, 'classifier__gamma': 4.713190292505617, 'classifier__reg_alpha': 0.8672643810586234, 'classifier__reg_lambda': 0.2975000969830429, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 0 with value: 0.4790884096792635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:38:41,668] Trial 1 finished with value: 0.5324719765148741 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.07876230678684952, 'classifier__n_estimators': 664, 'classifier__subsample': 0.638745554403415, 'classifier__colsample_bytree': 0.5759160641486623, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 1.3529706394092873, 'classifier__reg_alpha': 0.5929729556833648, 'classifier__reg_lambda': 0.02623189935979564, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 1 with value: 0.5324719765148741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:39:22,806] Trial 2 finished with value: 0.5320173697429869 and parameters: {'classifier__max_depth': 4, 'classifier__learning_rate': 0.08636846777461826, 'classifier__n_estimators': 445, 'classifier__subsample': 0.8218283121179062, 'classifier__colsample_bytree': 0.9092525231579498, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 10, 'classifier__gamma': 1.3719660637530695, 'classifier__reg_alpha': 0.03499839469483834, 'classifier__reg_lambda': 0.6556557727225284, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 1 with value: 0.5324719765148741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:39:51,723] Trial 3 finished with value: 0.5085546626582644 and parameters: {'classifier__max_depth': 4, 'classifier__learning_rate': 0.03384604254392903, 'classifier__n_estimators': 212, 'classifier__subsample': 0.9637043849094982, 'classifier__colsample_bytree': 0.9249675440810066, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 3.536512107339517, 'classifier__reg_alpha': 0.7634635063837841, 'classifier__reg_lambda': 0.2591190427813993, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 1 with value: 0.5324719765148741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:40:46,272] Trial 4 finished with value: 0.516150135219338 and parameters: {'classifier__max_depth': 7, 'classifier__learning_rate': 0.04714354352703769, 'classifier__n_estimators': 564, 'classifier__subsample': 0.8192455225693711, 'classifier__colsample_bytree': 0.5523423622482937, 'classifier__min_child_weight': 50, 'classifier__scale_pos_weight': 100, 'classifier__gamma': 3.3694201690691394, 'classifier__reg_alpha': 0.8049707129648106, 'classifier__reg_lambda': 0.41577877475724445, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 1 with value: 0.5324719765148741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:41:34,903] Trial 5 finished with value: 0.5162629357692085 and parameters: {'classifier__max_depth': 5, 'classifier__learning_rate': 0.08198810943546014, 'classifier__n_estimators': 605, 'classifier__subsample': 0.8988754657334248, 'classifier__colsample_bytree': 0.9739410822358016, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 1.3466385458542807, 'classifier__reg_alpha': 0.46812051660400844, 'classifier__reg_lambda': 0.9577047847249246, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 1 with value: 0.5324719765148741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:43:06,011] Trial 6 finished with value: 0.46197420233438036 and parameters: {'classifier__max_depth': 9, 'classifier__learning_rate': 0.04644257166676603, 'classifier__n_estimators': 856, 'classifier__subsample': 0.8306548787307853, 'classifier__colsample_bytree': 0.994520673347145, 'classifier__min_child_weight': 5, 'classifier__scale_pos_weight': 100, 'classifier__gamma': 2.403999096985946, 'classifier__reg_alpha': 0.725393006150773, 'classifier__reg_lambda': 0.3360191326086789, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 1 with value: 0.5324719765148741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:44:16,278] Trial 7 finished with value: 0.5027754331194234 and parameters: {'classifier__max_depth': 9, 'classifier__learning_rate': 0.04365364649854218, 'classifier__n_estimators': 608, 'classifier__subsample': 0.8019870147401067, 'classifier__colsample_bytree': 0.814814956464776, 'classifier__min_child_weight': 10, 'classifier__scale_pos_weight': 10, 'classifier__gamma': 2.9109340741571255, 'classifier__reg_alpha': 0.7836033926478364, 'classifier__reg_lambda': 0.653316582189947, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 1 with value: 0.5324719765148741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:45:25,454] Trial 8 finished with value: 0.5018694956736235 and parameters: {'classifier__max_depth': 9, 'classifier__learning_rate': 0.03717395220661869, 'classifier__n_estimators': 645, 'classifier__subsample': 0.9530828378974809, 'classifier__colsample_bytree': 0.7695049703196162, 'classifier__min_child_weight': 5, 'classifier__scale_pos_weight': 10, 'classifier__gamma': 2.2634218889050866, 'classifier__reg_alpha': 0.951939771593769, 'classifier__reg_lambda': 0.35282360181452077, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 1 with value: 0.5324719765148741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:46:49,638] Trial 9 finished with value: 0.45075395219466446 and parameters: {'classifier__max_depth': 9, 'classifier__learning_rate': 0.09868511574303156, 'classifier__n_estimators': 788, 'classifier__subsample': 0.7647004143447615, 'classifier__colsample_bytree': 0.8945695210903714, 'classifier__min_child_weight': 10, 'classifier__scale_pos_weight': 100, 'classifier__gamma': 1.6453976441552698, 'classifier__reg_alpha': 0.8197463596522394, 'classifier__reg_lambda': 0.6964830619691308, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 1 with value: 0.5324719765148741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:47:44,982] Trial 10 finished with value: 0.536325181630726 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.06702919015291328, 'classifier__n_estimators': 993, 'classifier__subsample': 0.5532150932513342, 'classifier__colsample_bytree': 0.5962755459430756, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.030732392831078315, 'classifier__reg_alpha': 0.4463656438898628, 'classifier__reg_lambda': 0.022906113579239582, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 10 with value: 0.536325181630726.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:48:39,382] Trial 11 finished with value: 0.537231183313741 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.0620206759288799, 'classifier__n_estimators': 933, 'classifier__subsample': 0.5728809247735805, 'classifier__colsample_bytree': 0.6073525818450453, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.00014461266764675962, 'classifier__reg_alpha': 0.4381082548456169, 'classifier__reg_lambda': 0.000506022288171825, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 11 with value: 0.537231183313741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:49:34,086] Trial 12 finished with value: 0.5364382391294573 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.0663200397756924, 'classifier__n_estimators': 999, 'classifier__subsample': 0.5045298017994335, 'classifier__colsample_bytree': 0.649388222159961, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.0815019902047202, 'classifier__reg_alpha': 0.2936153004377325, 'classifier__reg_lambda': 0.01315791139072577, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 11 with value: 0.537231183313741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 12.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:50:58,393] Trial 13 finished with value: 0.5253302114046752 and parameters: {'classifier__max_depth': 6, 'classifier__learning_rate': 0.010461695378290899, 'classifier__n_estimators': 982, 'classifier__subsample': 0.5088110522199196, 'classifier__colsample_bytree': 0.6693953201336107, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.08770117552893897, 'classifier__reg_alpha': 0.2548615152900378, 'classifier__reg_lambda': 0.14444421472435967, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 11 with value: 0.537231183313741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 13.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:51:54,380] Trial 14 finished with value: 0.5171712499919703 and parameters: {'classifier__max_depth': 4, 'classifier__learning_rate': 0.06309434006042118, 'classifier__n_estimators': 844, 'classifier__subsample': 0.6387139444403724, 'classifier__colsample_bytree': 0.6762844990444157, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 500, 'classifier__gamma': 0.5928237846991278, 'classifier__reg_alpha': 0.27823229274144595, 'classifier__reg_lambda': 0.16313234118947095, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 11 with value: 0.537231183313741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 14.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:52:27,297] Trial 15 finished with value: 0.5321310053766549 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.0615704416397059, 'classifier__n_estimators': 406, 'classifier__subsample': 0.6188350254125262, 'classifier__colsample_bytree': 0.6604636011975294, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.6485527056831482, 'classifier__reg_alpha': 0.2818665665174723, 'classifier__reg_lambda': 0.1419610059937566, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 11 with value: 0.537231183313741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:53:23,265] Trial 16 finished with value: 0.5212506985797152 and parameters: {'classifier__max_depth': 5, 'classifier__learning_rate': 0.06975892605819603, 'classifier__n_estimators': 748, 'classifier__subsample': 0.6943863898968838, 'classifier__colsample_bytree': 0.5125994431068105, 'classifier__min_child_weight': 50, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.7537114417473494, 'classifier__reg_alpha': 0.10438013007078417, 'classifier__reg_lambda': 0.0021196004737912685, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 11 with value: 0.537231183313741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:54:36,463] Trial 17 finished with value: 0.5049293711819005 and parameters: {'classifier__max_depth': 6, 'classifier__learning_rate': 0.05489414151438665, 'classifier__n_estimators': 906, 'classifier__subsample': 0.5579299533074815, 'classifier__colsample_bytree': 0.7161063886163167, 'classifier__min_child_weight': 5, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 4.643981086891748, 'classifier__reg_alpha': 0.37634026008861754, 'classifier__reg_lambda': 0.5462884609941473, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 11 with value: 0.537231183313741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 17.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:55:48,102] Trial 18 finished with value: 0.5062892730274358 and parameters: {'classifier__max_depth': 7, 'classifier__learning_rate': 0.011946575796439332, 'classifier__n_estimators': 732, 'classifier__subsample': 0.5028840349015989, 'classifier__colsample_bytree': 0.6160569030016005, 'classifier__min_child_weight': 10, 'classifier__scale_pos_weight': 500, 'classifier__gamma': 0.7725640711402311, 'classifier__reg_alpha': 0.6148880035531685, 'classifier__reg_lambda': 0.8508658799676784, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 11 with value: 0.537231183313741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:56:53,677] Trial 19 finished with value: 0.5064028444238885 and parameters: {'classifier__max_depth': 5, 'classifier__learning_rate': 0.07420880481104691, 'classifier__n_estimators': 925, 'classifier__subsample': 0.580521361064176, 'classifier__colsample_bytree': 0.7602799558056688, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 1.8976349213653496, 'classifier__reg_alpha': 0.18410467813251208, 'classifier__reg_lambda': 0.1629589360987062, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 11 with value: 0.537231183313741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 19.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:57:28,847] Trial 20 finished with value: 0.5312246182703487 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.05627419314085936, 'classifier__n_estimators': 450, 'classifier__subsample': 0.7047777534596977, 'classifier__colsample_bytree': 0.6332201912459462, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.04345213600829957, 'classifier__reg_alpha': 0.5784652750981585, 'classifier__reg_lambda': 0.4762735169330788, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 11 with value: 0.537231183313741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:58:21,638] Trial 21 finished with value: 0.5342846864902713 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.0690839391322935, 'classifier__n_estimators': 955, 'classifier__subsample': 0.5542189149581334, 'classifier__colsample_bytree': 0.5897227488047166, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.018989613660380433, 'classifier__reg_alpha': 0.38480395420173624, 'classifier__reg_lambda': 0.0629526535724273, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 11 with value: 0.537231183313741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 21.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 04:59:23,922] Trial 22 finished with value: 0.5295243876587462 and parameters: {'classifier__max_depth': 4, 'classifier__learning_rate': 0.06364910516669033, 'classifier__n_estimators': 970, 'classifier__subsample': 0.5874863655890243, 'classifier__colsample_bytree': 0.5001554002194328, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.3418896826581008, 'classifier__reg_alpha': 0.4448068784226762, 'classifier__reg_lambda': 0.06986403048106728, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 11 with value: 0.537231183313741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 22.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:00:17,929] Trial 23 finished with value: 0.5375717047914538 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.05491919191787746, 'classifier__n_estimators': 850, 'classifier__subsample': 0.5310319052145951, 'classifier__colsample_bytree': 0.7081879908376594, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 1.0446454793119093, 'classifier__reg_alpha': 0.3716352161480184, 'classifier__reg_lambda': 0.23266318591810042, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 23 with value: 0.5375717047914538.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 23.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:01:14,422] Trial 24 finished with value: 0.5338314286999031 and parameters: {'classifier__max_depth': 4, 'classifier__learning_rate': 0.024909426308637097, 'classifier__n_estimators': 824, 'classifier__subsample': 0.5033170094656049, 'classifier__colsample_bytree': 0.7051087924112063, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 1.1210375497777811, 'classifier__reg_alpha': 0.35554020846812984, 'classifier__reg_lambda': 0.2468150519397922, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 23 with value: 0.5375717047914538.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:02:06,849] Trial 25 finished with value: 0.5376848907646156 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.057205044854145085, 'classifier__n_estimators': 880, 'classifier__subsample': 0.6817350047377985, 'classifier__colsample_bytree': 0.8014912628877667, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.9527946803718744, 'classifier__reg_alpha': 0.18995150681951026, 'classifier__reg_lambda': 0.212947697608099, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 25 with value: 0.5376848907646156.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 25.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:03:03,803] Trial 26 finished with value: 0.5146771116378563 and parameters: {'classifier__max_depth': 5, 'classifier__learning_rate': 0.05113862693756324, 'classifier__n_estimators': 721, 'classifier__subsample': 0.6856877473788642, 'classifier__colsample_bytree': 0.8135372686568447, 'classifier__min_child_weight': 5, 'classifier__scale_pos_weight': 500, 'classifier__gamma': 1.0214149609988143, 'classifier__reg_alpha': 0.15738746132029835, 'classifier__reg_lambda': 0.2180063082362747, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 25 with value: 0.5376848907646156.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 26.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:03:59,834] Trial 27 finished with value: 0.5363244750213588 and parameters: {'classifier__max_depth': 4, 'classifier__learning_rate': 0.03639779491925464, 'classifier__n_estimators': 829, 'classifier__subsample': 0.7408266224253841, 'classifier__colsample_bytree': 0.8041895536610708, 'classifier__min_child_weight': 10, 'classifier__scale_pos_weight': 10, 'classifier__gamma': 1.919272326390511, 'classifier__reg_alpha': 8.078948623657656e-05, 'classifier__reg_lambda': 0.4078662672015704, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 25 with value: 0.5376848907646156.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 27.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:04:51,250] Trial 28 finished with value: 0.5329251700680271 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.05776703968713713, 'classifier__n_estimators': 895, 'classifier__subsample': 0.6066083514936784, 'classifier__colsample_bytree': 0.7295459630777962, 'classifier__min_child_weight': 50, 'classifier__scale_pos_weight': 100, 'classifier__gamma': 0.4544519053974696, 'classifier__reg_alpha': 0.5512639016103412, 'classifier__reg_lambda': 0.1267282901221839, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 25 with value: 0.5376848907646156.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:05:34,281] Trial 29 finished with value: 0.4907623672698541 and parameters: {'classifier__max_depth': 8, 'classifier__learning_rate': 0.07572905396459054, 'classifier__n_estimators': 318, 'classifier__subsample': 0.665046921464763, 'classifier__colsample_bytree': 0.8393061897374914, 'classifier__min_child_weight': 50, 'classifier__scale_pos_weight': 500, 'classifier__gamma': 0.9217892160535283, 'classifier__reg_alpha': 0.1901682018133961, 'classifier__reg_lambda': 0.2916303236358751, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 25 with value: 0.5376848907646156.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 29.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:05:57,832] Trial 30 finished with value: 0.4828285572963841 and parameters: {'classifier__max_depth': 4, 'classifier__learning_rate': 0.027457579681049372, 'classifier__n_estimators': 127, 'classifier__subsample': 0.736327174827377, 'classifier__colsample_bytree': 0.8548963236315748, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 2.865804572041687, 'classifier__reg_alpha': 0.6700952312703, 'classifier__reg_lambda': 0.4883160093126434, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 25 with value: 0.5376848907646156.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:06:51,850] Trial 31 finished with value: 0.5367781824722335 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.05104816548916989, 'classifier__n_estimators': 890, 'classifier__subsample': 0.5307360224820229, 'classifier__colsample_bytree': 0.643235251652379, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.44618932861633465, 'classifier__reg_alpha': 0.3375317742806647, 'classifier__reg_lambda': 0.08789297175376029, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 25 with value: 0.5376848907646156.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 31.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:07:45,339] Trial 32 finished with value: 0.5380248341073918 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.05234450595039155, 'classifier__n_estimators': 899, 'classifier__subsample': 0.5410434450411552, 'classifier__colsample_bytree': 0.6966875820615125, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.35671465038041017, 'classifier__reg_alpha': 0.36971142945786795, 'classifier__reg_lambda': 0.0850666877231463, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:08:34,908] Trial 33 finished with value: 0.5363255670540171 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.040733764413645665, 'classifier__n_estimators': 788, 'classifier__subsample': 0.6363937123119944, 'classifier__colsample_bytree': 0.694641637021058, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 1.251874422544306, 'classifier__reg_alpha': 0.49363758960849274, 'classifier__reg_lambda': 0.21118616639811533, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 33.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:09:25,723] Trial 34 finished with value: 0.5307716816660564 and parameters: {'classifier__max_depth': 4, 'classifier__learning_rate': 0.051232858826933694, 'classifier__n_estimators': 692, 'classifier__subsample': 0.5824514956859731, 'classifier__colsample_bytree': 0.7763522221833913, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 1.5467623361751783, 'classifier__reg_alpha': 0.40377859678728967, 'classifier__reg_lambda': 0.09089851383510567, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 34.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:10:13,915] Trial 35 finished with value: 0.5370054537395694 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.05937522737567157, 'classifier__n_estimators': 770, 'classifier__subsample': 0.6598813444414932, 'classifier__colsample_bytree': 0.5468357897427698, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.42992240369042634, 'classifier__reg_alpha': 0.5103656581533695, 'classifier__reg_lambda': 0.2021294328188713, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 35.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:11:13,879] Trial 36 finished with value: 0.5316774906374259 and parameters: {'classifier__max_depth': 4, 'classifier__learning_rate': 0.05398855392363187, 'classifier__n_estimators': 883, 'classifier__subsample': 0.5396926889397601, 'classifier__colsample_bytree': 0.735747731273226, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.952573761213889, 'classifier__reg_alpha': 0.0629893530167569, 'classifier__reg_lambda': 0.3192152212356215, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 36.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:12:06,931] Trial 37 finished with value: 0.5320176266918477 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.08667628655851319, 'classifier__n_estimators': 936, 'classifier__subsample': 0.6072422736194099, 'classifier__colsample_bytree': 0.7898797572394032, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 10, 'classifier__gamma': 1.79684503136306, 'classifier__reg_alpha': 0.12843550475247906, 'classifier__reg_lambda': 0.27174036484711056, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 37.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:12:52,460] Trial 38 finished with value: 0.5258978114380785 and parameters: {'classifier__max_depth': 5, 'classifier__learning_rate': 0.04711088632745041, 'classifier__n_estimators': 528, 'classifier__subsample': 0.8613184849245767, 'classifier__colsample_bytree': 0.561587508370121, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 100, 'classifier__gamma': 4.437368371133057, 'classifier__reg_alpha': 0.23300176336639755, 'classifier__reg_lambda': 0.4014556786362924, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 38.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:13:46,640] Trial 39 finished with value: 0.5243114091717895 and parameters: {'classifier__max_depth': 4, 'classifier__learning_rate': 0.07916830008323403, 'classifier__n_estimators': 816, 'classifier__subsample': 0.5752919158071065, 'classifier__colsample_bytree': 0.7449896783233936, 'classifier__min_child_weight': 50, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.3050487754755993, 'classifier__reg_alpha': 0.31783648259469083, 'classifier__reg_lambda': 0.09637444405383731, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 39.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:14:56,636] Trial 40 finished with value: 0.5139971607150887 and parameters: {'classifier__max_depth': 6, 'classifier__learning_rate': 0.042626001920102886, 'classifier__n_estimators': 868, 'classifier__subsample': 0.5306791117186461, 'classifier__colsample_bytree': 0.690634721221265, 'classifier__min_child_weight': 5, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 1.5127497731676718, 'classifier__reg_alpha': 0.22054955103521548, 'classifier__reg_lambda': 0.5697396574974984, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 40.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:15:44,583] Trial 41 finished with value: 0.5365517462886948 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.057750414317217134, 'classifier__n_estimators': 791, 'classifier__subsample': 0.6734025904407472, 'classifier__colsample_bytree': 0.5346237206978597, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.29721523431440333, 'classifier__reg_alpha': 0.5378276090503386, 'classifier__reg_lambda': 0.1884681429599911, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 41.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:16:32,623] Trial 42 finished with value: 0.5354186018127742 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.06031094076590467, 'classifier__n_estimators': 767, 'classifier__subsample': 0.6598321642171386, 'classifier__colsample_bytree': 0.6182243848670163, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.6140321366819862, 'classifier__reg_alpha': 0.43640218492750743, 'classifier__reg_lambda': 0.23333153006250515, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 42.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:17:17,274] Trial 43 finished with value: 0.5356442671497306 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.0725663077499137, 'classifier__n_estimators': 683, 'classifier__subsample': 0.7219987571546949, 'classifier__colsample_bytree': 0.9328859513576059, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 3.814107118860539, 'classifier__reg_alpha': 0.5162128669610364, 'classifier__reg_lambda': 0.050696716536402064, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 43.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:18:16,695] Trial 44 finished with value: 0.5351914590198686 and parameters: {'classifier__max_depth': 4, 'classifier__learning_rate': 0.04886868394149145, 'classifier__n_estimators': 932, 'classifier__subsample': 0.6236373697601605, 'classifier__colsample_bytree': 0.5352795748395032, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 10, 'classifier__gamma': 1.328027139819377, 'classifier__reg_alpha': 0.6636010894443403, 'classifier__reg_lambda': 0.3565298472853179, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 44.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:18:58,095] Trial 45 finished with value: 0.537344818947409 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.06513195237572632, 'classifier__n_estimators': 634, 'classifier__subsample': 0.7785996952745035, 'classifier__colsample_bytree': 0.5771146690777876, 'classifier__min_child_weight': 10, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.8795423292650257, 'classifier__reg_alpha': 0.41469540837652785, 'classifier__reg_lambda': 0.11984709444226907, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 45.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:19:37,039] Trial 46 finished with value: 0.5315648185619857 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.06496568445296363, 'classifier__n_estimators': 545, 'classifier__subsample': 0.7616769633505771, 'classifier__colsample_bytree': 0.6091763779848918, 'classifier__min_child_weight': 10, 'classifier__scale_pos_weight': 100, 'classifier__gamma': 2.2734470069106663, 'classifier__reg_alpha': 0.3265828440354343, 'classifier__reg_lambda': 0.11623791153068218, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 46.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:20:37,236] Trial 47 finished with value: 0.48407533740597286 and parameters: {'classifier__max_depth': 7, 'classifier__learning_rate': 0.09835026706778992, 'classifier__n_estimators': 653, 'classifier__subsample': 0.7740196770337535, 'classifier__colsample_bytree': 0.575605085125768, 'classifier__min_child_weight': 10, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 1.1459446847428265, 'classifier__reg_alpha': 0.4168303196272795, 'classifier__reg_lambda': 0.032847207012916324, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 47.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:21:20,912] Trial 48 finished with value: 0.5321314550371612 and parameters: {'classifier__max_depth': 4, 'classifier__learning_rate': 0.0697647843778962, 'classifier__n_estimators': 600, 'classifier__subsample': 0.7949377224049576, 'classifier__colsample_bytree': 0.6791619558304807, 'classifier__min_child_weight': 10, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.8660707473403996, 'classifier__reg_alpha': 0.47891542428776096, 'classifier__reg_lambda': 0.18166482069907292, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 48.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-09 05:22:11,400] Trial 49 finished with value: 0.5328106351133466 and parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.04496097576863797, 'classifier__n_estimators': 853, 'classifier__subsample': 0.8483380558727407, 'classifier__colsample_bytree': 0.8602913292657859, 'classifier__min_child_weight': 10, 'classifier__scale_pos_weight': 500, 'classifier__gamma': 0.2246049274320443, 'classifier__reg_alpha': 0.3689568444588318, 'classifier__reg_lambda': 0.0031195170257367066, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}. Best is trial 32 with value: 0.5380248341073918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study for XGBoost saved after trial 49.\n",
      "Best parameters for XGBoost: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.05234450595039155, 'classifier__n_estimators': 899, 'classifier__subsample': 0.5410434450411552, 'classifier__colsample_bytree': 0.6966875820615125, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.35671465038041017, 'classifier__reg_alpha': 0.36971142945786795, 'classifier__reg_lambda': 0.0850666877231463, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}\n",
      "Best score for XGBoost: 0.5380248341073918\n",
      "Final study for XGBoost saved as saved_models/study_XGBoost.joblib.\n",
      "\n",
      "\n",
      "--- All Models Processed ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "from scipy.stats import randint, uniform\n",
    "from scipy.stats._distn_infrastructure import rv_frozen  # For checking scipy distributions\n",
    "from scipy.stats import rv_continuous, rv_discrete\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# Perform stratified cross-validation\n",
    "scores = []\n",
    "for train_idx, val_idx in stratified_cv.split(X_train, y_train):\n",
    "    # Use iloc for positional indexing\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    pipeline.fit(X_train_fold, y_train_fold)\n",
    "    y_val_scores = pipeline.predict_proba(X_val_fold)[:, 1]\n",
    "    score = recall_at_fpr(y_val_fold, y_val_scores, target_fpr=0.05)\n",
    "    scores.append(score)\n",
    "\n",
    "\n",
    "# Optuna optimization for each model\n",
    "best_models = {}\n",
    "search_results = {}\n",
    "\n",
    "def save_study_progress(study, filename):\n",
    "    \"\"\"Save the study progress to a file.\"\"\"\n",
    "    joblib.dump(study, filename)\n",
    "\n",
    "\n",
    "def objective(trial, model_name, model, param_distributions, X_train, y_train):\n",
    "    # Define hyperparameters for the trial\n",
    "    params = {}\n",
    "    for key, value in param_distributions.items():\n",
    "        if isinstance(value, list):  # Handle list values\n",
    "            params[key.split(\"__\")[1]] = trial.suggest_categorical(key, value)\n",
    "        elif isinstance(value, rv_frozen):  # Handle scipy.stats frozen distributions\n",
    "            if isinstance(value.dist, rv_discrete):  # Check if it is a discrete distribution\n",
    "                low = value.kwds.get(\"low\", None)\n",
    "                high = value.kwds.get(\"high\", None)\n",
    "                if low is None or high is None:\n",
    "                    raise ValueError(f\"Invalid randint distribution for {key}. 'low' or 'high' is missing.\")\n",
    "                params[key.split(\"__\")[1]] = trial.suggest_int(key, int(low), int(high - 1))\n",
    "            elif isinstance(value.dist, rv_continuous):  # Check if it is a continuous distribution\n",
    "                loc = value.kwds.get(\"loc\", None)\n",
    "                scale = value.kwds.get(\"scale\", None)\n",
    "                if loc is None or scale is None:\n",
    "                    raise ValueError(f\"Invalid uniform distribution for {key}. 'loc' or 'scale' is missing.\")\n",
    "                params[key.split(\"__\")[1]] = trial.suggest_float(key, float(loc), float(loc + scale))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported scipy.stats distribution: {type(value.dist)}\")\n",
    "        elif isinstance(value, tuple) and len(value) == 2:  # Handle (low, high) tuples\n",
    "            params[key.split(\"__\")[1]] = trial.suggest_float(key, value[0], value[1])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported parameter type: {type(value)}\")\n",
    "\n",
    "    # Create the pipeline with the trial's parameters\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model.set_params(**params))])\n",
    "\n",
    "    # Perform stratified cross-validation\n",
    "    scores = []\n",
    "    for train_idx, val_idx in stratified_cv.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "\n",
    "        pipeline.fit(X_train_fold, y_train_fold)\n",
    "        y_val_scores = pipeline.predict_proba(X_val_fold)[:, 1]\n",
    "        score = recall_at_fpr(y_val_fold, y_val_scores, target_fpr=0.05)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Optimize selected models\n",
    "for name in models:\n",
    "    model_config = models[name]\n",
    "    model = model_config[\"model\"]\n",
    "    param_distributions = model_config[\"param_distributions\"]\n",
    "    print(f\"\\n--- Starting Optuna optimization for {name} ---\\n\")\n",
    "\n",
    "    # Instantiate the model based on the model name\n",
    "    if name == \"LightGBM\":\n",
    "        model = LGBMClassifier()\n",
    "    elif name == \"XGBoost\":\n",
    "        model = XGBClassifier()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {name}\")\n",
    "\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Define the study name and filename\n",
    "    study_name = f\"study_{name}\"\n",
    "    filename = f\"saved_models/{study_name}.joblib\"\n",
    "\n",
    "    # Load existing study or create a new one\n",
    "    if os.path.exists(filename):\n",
    "        study = joblib.load(filename)\n",
    "        print(f\"Loaded existing study for {name}.\")\n",
    "    else:\n",
    "        study = optuna.create_study(direction=\"maximize\", study_name=study_name)\n",
    "        print(f\"Created new study for {name}.\")\n",
    "\n",
    "    # Define a trial callback to save progress after each trial\n",
    "    def trial_callback(study, trial):\n",
    "        save_study_progress(study, filename)\n",
    "        print(f\"Study for {name} saved after trial {trial.number}.\")\n",
    "\n",
    "    # Optimize the study\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, name, model, param_distributions, X_train, y_train),\n",
    "        n_trials=50,\n",
    "        callbacks=[trial_callback]\n",
    "    )\n",
    "    \n",
    "    # Get the best parameters and model\n",
    "    best_params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    print(f\"Best parameters for {name}: {best_params}\")\n",
    "    print(f\"Best score for {name}: {best_score}\")\n",
    "\n",
    "    # Save the final study\n",
    "    save_study_progress(study, filename)\n",
    "    print(f\"Final study for {name} saved as {filename}.\\n\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    elapsed_time_str = f\"{elapsed_time:.2f}s\"\n",
    "    \n",
    "    best_models[name] = (model.set_params(**best_params), best_score)\n",
    "\n",
    "# Optimize LightGBM using Optuna\n",
    "study_lgbm = optuna.create_study(direction='maximize', storage=storage, study_name='lgbm_study', load_if_exists=True, sampler=sampler, pruner=pruner)\n",
    "print(\"\\n--- Starting Optuna optimization for LightGBM ---\\n\")\n",
    "total_trials = 200\n",
    "start_time = time.time()\n",
    "for trial_idx in range(total_trials):\n",
    "    trial_start_time = time.time()\n",
    "    study_lgbm.optimize(objective_lgbm, n_trials=1)\n",
    "    # Save intermediate model after each trial\n",
    "    current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    intermediate_lgbm_filename = f\"saved_models/LightGBM_trial_{trial_idx + 1}_{current_datetime}.joblib\"\n",
    "    print(f\"Saving intermediate LightGBM model for trial {trial_idx + 1} as {intermediate_lgbm_filename}...\\n\")\n",
    "    joblib.dump(pipeline_lgbm, intermediate_lgbm_filename)\n",
    "    print(f\"Intermediate LightGBM model for trial {trial_idx + 1} saved successfully as {intermediate_lgbm_filename}.\\n\")\n",
    "    trial_end_time = time.time()\n",
    "    trial_duration = trial_end_time - trial_start_time\n",
    "    elapsed_time = trial_end_time - start_time\n",
    "    remaining_trials = total_trials - (trial_idx + 1)\n",
    "    eta_trials = remaining_trials * trial_duration\n",
    "    print(f\"\\n--- Finished trial {trial_idx + 1}/{total_trials} for LightGBM in {trial_duration:.2f} seconds ---\\n\")\n",
    "    print(f\"Elapsed time: {elapsed_time:.2f} seconds. Estimated time remaining for all trials: {eta_trials:.2f} seconds\\n\")\n",
    "end_time = time.time()\n",
    "print(f\"\\n--- Finished Optuna optimization for LightGBM in {end_time - start_time:.2f} seconds ---\\n\")\n",
    "\n",
    "# Save the best LightGBM model\n",
    "lgbm_best_params = study_lgbm.best_params\n",
    "pipeline_lgbm.set_params(**lgbm_best_params)\n",
    "pipeline_lgbm.fit(X_train, y_train)\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "lgbm_filename = f\"saved_models/LightGBM_optuna_{current_datetime}.joblib\"\n",
    "print(f\"Saving best Optuna LightGBM model as {lgbm_filename}...\\n\")\n",
    "joblib.dump(pipeline_lgbm, lgbm_filename)\n",
    "print(f\"Best Optuna LightGBM model saved successfully as {lgbm_filename}.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__max_depth': 3, 'classifier__learning_rate': 0.05234450595039155, 'classifier__n_estimators': 899, 'classifier__subsample': 0.5410434450411552, 'classifier__colsample_bytree': 0.6966875820615125, 'classifier__min_child_weight': 1, 'classifier__scale_pos_weight': 50, 'classifier__gamma': 0.35671465038041017, 'classifier__reg_alpha': 0.36971142945786795, 'classifier__reg_lambda': 0.0850666877231463, 'classifier__objective': 'binary:logistic', 'classifier__eval_metric': 'aucpr'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gamacore/FINAL/Bank-Fraud-Detection/bank/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:04:42] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"classifier__colsample_bytree\", \"classifier__eval_metric\", \"classifier__gamma\", \"classifier__learning_rate\", \"classifier__max_depth\", \"classifier__min_child_weight\", \"classifier__n_estimators\", \"classifier__objective\", \"classifier__reg_alpha\", \"classifier__reg_lambda\", \"classifier__scale_pos_weight\", \"classifier__subsample\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC Score: 0.9878\n",
      "Recall at FPR 1.0%: 0.78\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def recall_at_fpr(y_true, y_scores, target_fpr):\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    idx = np.where(fpr <= target_fpr)[0][-1] if np.any(fpr <= target_fpr) else None\n",
    "    return tpr[idx] if idx is not None else 0.0\n",
    "\n",
    "# Load the Optuna study and get the best parameters\n",
    "study = joblib.load(\"./saved_models/study_XGBoost.joblib\")\n",
    "best_params = study.best_params\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Create the best model using the best parameters\n",
    "best_model = XGBClassifier(**best_params)\n",
    "\n",
    "# Train the model\n",
    "best_model.fit(X_test_preprocessed, y_test)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, \"best_xgb_model.joblib\")\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = joblib.load(\"best_xgb_model.joblib\")\n",
    "\n",
    "# Predict probabilities for evaluation\n",
    "probabilities = loaded_model.predict_proba(X_test_preprocessed)[:, 1]\n",
    "\n",
    "# Calculate AUC-ROC\n",
    "auc_score = roc_auc_score(y_test, probabilities)\n",
    "\n",
    "# Evaluate recall at a target FPR\n",
    "target_fpr = 0.01  # Specify the desired FPR\n",
    "recall = recall_at_fpr(y_test, probabilities, target_fpr)\n",
    "\n",
    "print(f\"AUC-ROC Score: {auc_score:.4f}\")\n",
    "print(f\"Recall at FPR {target_fpr * 100:.1f}%: {recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test data evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now review the performance of the best model on the test data. We will use the following metrics to evaluate the model: auc roc score, tpr @ fpr = 0.05, and predictive equality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'transformers_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Decode protected attributes\u001b[39;00m\n\u001b[1;32m     70\u001b[0m attributes_to_decode \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_age\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memployment_status\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincome\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 71\u001b[0m protected_attributes \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_protected_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_preprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes_to_decode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Load Optuna studies\u001b[39;00m\n\u001b[1;32m     74\u001b[0m lightgbm_study_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./saved_models/study_LightGBM.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mdecode_protected_attributes\u001b[0;34m(X_original, X_preprocessed, preprocessor, attributes)\u001b[0m\n\u001b[1;32m     19\u001b[0m decoded_attributes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attributes:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformers_\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39monehot_features:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;66;03m# Decode one-hot-encoded attributes\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         encoder \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mtransformers_[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39monehot_encoder\n\u001b[1;32m     25\u001b[0m         encoded_columns \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mget_feature_names_out([attr])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'transformers_'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def decode_protected_attributes(X_original, X_preprocessed, preprocessor, attributes):\n",
    "    \"\"\"\n",
    "    Decode the protected attributes from preprocessed data back to their original values.\n",
    "\n",
    "    Parameters:\n",
    "        X_original: The original unprocessed DataFrame.\n",
    "        X_preprocessed: The preprocessed array.\n",
    "        preprocessor: The ColumnTransformer used for preprocessing.\n",
    "        attributes: List of attribute names to decode.\n",
    "\n",
    "    Returns:\n",
    "        dict: Decoded attributes with original values for fairness evaluation.\n",
    "    \"\"\"\n",
    "    # Initialize dictionary to hold decoded attributes\n",
    "    decoded_attributes = {}\n",
    "\n",
    "    for attr in attributes:\n",
    "        if attr in preprocessor.transformers_[0][1].onehot_features:\n",
    "            # Decode one-hot-encoded attributes\n",
    "            encoder = preprocessor.transformers_[0][1].onehot_encoder\n",
    "            encoded_columns = encoder.get_feature_names_out([attr])\n",
    "            indices = [\n",
    "                i for i, col in enumerate(preprocessor.transformers_[0][1].get_feature_names_out())\n",
    "                if col in encoded_columns\n",
    "            ]\n",
    "            decoded_values = np.argmax(X_preprocessed[:, indices], axis=1)\n",
    "            decoded_attributes[attr] = encoder.categories_[0][decoded_values]\n",
    "        elif attr in preprocessor.transformers_[0][1].ordinal_features:\n",
    "            # Decode ordinal-encoded attributes\n",
    "            encoder = preprocessor.transformers_[0][1].ordinal_encoder\n",
    "            index = list(preprocessor.transformers_[0][1].ordinal_features).index(attr)\n",
    "            decoded_values = X_preprocessed[:, index].astype(int)\n",
    "            decoded_attributes[attr] = encoder.categories_[0][decoded_values]\n",
    "        else:\n",
    "            raise ValueError(f\"Attribute {attr} is not recognized as encoded by the preprocessor.\")\n",
    "    return decoded_attributes\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, protected_attributes):\n",
    "    \"\"\"\n",
    "    Evaluate a model using AUC ROC, TPR @ FPR = 0.05, and Predictive Equality across multiple attributes.\n",
    "    \"\"\"\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # AUC ROC Score\n",
    "    auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # TPR @ FPR = 0.05\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    target_fpr = 0.05\n",
    "    tpr_at_fpr = tpr[np.where(fpr <= target_fpr)[0][-1]] if np.any(fpr <= target_fpr) else 0.0\n",
    "    \n",
    "    # Predictive Equality\n",
    "    fairness_results = {}\n",
    "    for attr_name, attr_values in protected_attributes.items():\n",
    "        groups = np.unique(attr_values)\n",
    "        fpr_groups = {}\n",
    "        for group in groups:\n",
    "            group_idx = attr_values == group\n",
    "            group_fpr, _, _ = roc_curve(y_test[group_idx], y_pred_proba[group_idx])\n",
    "            fpr_groups[group] = group_fpr[1] if len(group_fpr) > 1 else 0.0\n",
    "        fairness_results[attr_name] = fpr_groups\n",
    "\n",
    "    return {\"AUC ROC\": auc_roc, \"TPR @ FPR = 0.05\": tpr_at_fpr, \"Predictive Equality\": fairness_results}\n",
    "\n",
    "# Decode protected attributes\n",
    "attributes_to_decode = ['customer_age', 'employment_status', 'income']\n",
    "protected_attributes = decode_protected_attributes(X_test, X_test_preprocessed, preprocessor, attributes_to_decode)\n",
    "\n",
    "# Load Optuna studies\n",
    "lightgbm_study_path = './saved_models/study_LightGBM.joblib'\n",
    "xgboost_study_path = './saved_models/study_XGBoost.joblib'\n",
    "\n",
    "# Load and reconstruct LightGBM\n",
    "try:\n",
    "    lightgbm_study = joblib.load(lightgbm_study_path)\n",
    "    lightgbm_best_params = lightgbm_study.best_params\n",
    "    lightgbm_model = lgb.LGBMClassifier(**lightgbm_best_params)\n",
    "    lightgbm_model.fit(X_train_preprocessed, y_train)\n",
    "    print(\"LightGBM model reconstructed and trained.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with LightGBM model: {e}\")\n",
    "    lightgbm_model = None\n",
    "\n",
    "# Load and reconstruct XGBoost\n",
    "try:\n",
    "    xgboost_study = joblib.load(xgboost_study_path)\n",
    "    xgboost_best_params = xgboost_study.best_params\n",
    "    xgboost_model = xgb.XGBClassifier(**xgboost_best_params, use_label_encoder=False)\n",
    "    xgboost_model.fit(X_train_preprocessed, y_train)\n",
    "    print(\"XGBoost model reconstructed and trained.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with XGBoost model: {e}\")\n",
    "    xgboost_model = None\n",
    "\n",
    "# Evaluate models\n",
    "if lightgbm_model:\n",
    "    print(\"\\n--- Evaluating LightGBM Model ---\")\n",
    "    lightgbm_results = evaluate_model(lightgbm_model, X_test_preprocessed, y_test, protected_attributes)\n",
    "    print(lightgbm_results)\n",
    "\n",
    "if xgboost_model:\n",
    "    print(\"\\n--- Evaluating XGBoost Model ---\")\n",
    "    xgboost_results = evaluate_model(xgboost_model, X_test_preprocessed, y_test, protected_attributes)\n",
    "    print(xgboost_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
